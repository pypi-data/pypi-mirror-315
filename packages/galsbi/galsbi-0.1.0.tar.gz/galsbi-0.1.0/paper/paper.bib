@article{ufig,
	title        = {An Ultra Fast Image Generator (UFig) for wide-field astronomy},
	author       = {Bergé, Joel and Gamper, Lukas and Réfrégier, Alexandre and Amara, Adam},
	year         = 2013,
	month        = feb,
	journal      = {Astronomy and Computing},
	volume       = 1,
	pages        = {23–32},
	doi          = {10.1016/j.ascom.2013.01.001},
	issn         = {2213-1337},
	abstractnote = {Simulated wide-field images are becoming an important part of observational astronomy, either to prepare for new surveys or to test measurement methods. In order to efficiently explore vast parameter spaces, the computational speed of simulation codes is a central requirement to their implementation. We introduce the Ultra Fast Image Generator (UFig) which aims to bring wide-field imaging simulations to the current limits of computational capabilities. We achieve this goal through: (1) models of galaxies, stars and observational conditions, which, while simple, capture the key features necessary for realistic simulations, and (2) state-of-the-art computational and implementation optimizations. We present the performances of UFig and show that it is faster than existing public simulation codes by several orders of magnitude. It allows us to produce images more quickly than SExtractor needs to analyze them. For instance, it can simulate a typical 0.25deg2 Subaru SuprimeCam image (10k×8k pixels) with a 5-σ limiting magnitude of R=26 in 30 s on a laptop, yielding an average simulation time for a galaxy of 30 μs. This code is complementary to end-to-end simulation codes and can be used as a fast, central component of observational methods relying on simulations. For instance, it can be used to efficiently calibrate high-precision measurements, as recently suggested for cosmic shear.}
}
@unpublished{ufig2,
	title        = {UFig v1: The ultra-fast image generator},
	author       = {Fischbacher, Silvan and Moser, Beatrice and Kacprzak, Tomasz and Tortorelli, Luca and Herbel, Joerg and Schmitt, Uwe and Refregier, Alexandre and Berge, Joel and Gamper, Lukas and Bruderer, Claudio and Amara, Adam},
	year         = 2024,
	note         = {Manuscript in preparation}
}
@unpublished{fischbacher,
	title        = {GalSBI: Phenomenological galaxy population model for cosmology with simulations-based inference},
	author       = {Fischbacher, Silvan and Kacprzak, Tomasz and Tortorelli, Luca and Moser, Beatrice and Refregier, Alexandre and Gebhardt, Patrick and Gruen, Daniel},
	year         = 2024,
	note         = {Manuscript in preparation}
}
@article{herbel,
	title        = {The redshift distribution of cosmological samples: a forward modeling approach},
	author       = {Herbel, Jörg and Kacprzak, Tomasz and Amara, Adam and Refregier, Alexandre and Bruderer, Claudio and Nicola, Andrina},
	year         = 2017,
	month        = aug,
	journal      = {Journal of Cosmology and Astroparticle Physics},
	volume       = 2017,
	number       = {08},
	pages        = {035–035},
	doi          = {10.1088/1475-7516/2017/08/035},
	issn         = {1475-7516},
	note         = {arXiv: 1705.05386},
	abstractnote = {Determining the redshift distribution $n(z)$ of galaxy samples is essential for several cosmological probes including weak lensing. For imaging surveys, this is usually done using photometric redshifts estimated on an object-by-object basis. We present a new approach for directly measuring the global $n(z)$ of cosmological galaxy samples, including uncertainties, using forward modeling. Our method relies on image simulations produced using UFig (Ultra Fast Image Generator) and on ABC (Approximate Bayesian Computation) within the $MCCL$ (Monte-Carlo Control Loops) framework. The galaxy population is modeled using parametric forms for the luminosity functions, spectral energy distributions, sizes and radial profiles of both blue and red galaxies. We apply exactly the same analysis to the real data and to the simulated images, which also include instrumental and observational effects. By adjusting the parameters of the simulations, we derive a set of acceptable models that are statistically consistent with the data. We then apply the same cuts to the simulations that were used to construct the target galaxy sample in the real data. The redshifts of the galaxies in the resulting simulated samples yield a set of $n(z)$ distributions for the acceptable models. We demonstrate the method by determining $n(z)$ for a cosmic shear like galaxy sample from the 4-band Subaru Suprime-Cam data in the COSMOS field. We also complement this imaging data with a spectroscopic calibration sample from the VVDS survey. We compare our resulting posterior $n(z)$ distributions to the one derived from photometric redshifts estimated using 36 photometric bands in COSMOS and find good agreement. This offers good prospects for applying our approach to current and future large imaging surveys.}
}
@article{herbel_psf,
	title        = {Fast Point Spread Function Modeling with Deep Learning},
	author       = {Herbel, Jörg and Kacprzak, Tomasz and Amara, Adam and Refregier, Alexandre and Lucchi, Aurelien},
	year         = 2018,
	month        = jul,
	journal      = {Journal of Cosmology and Astroparticle Physics},
	volume       = 2018,
	number       = {07},
	pages        = {054–054},
	doi          = {10.1088/1475-7516/2018/07/054},
	issn         = {1475-7516},
	note         = {arXiv:1801.07615 [astro-ph, stat]},
	abstractnote = {Modeling the Point Spread Function (PSF) of wide-field surveys is vital for many astrophysical applications and cosmological probes including weak gravitational lensing. The PSF smears the image of any recorded object and therefore needs to be taken into account when inferring properties of galaxies from astronomical images. In the case of cosmic shear, the PSF is one of the dominant sources of systematic errors and must be treated carefully to avoid biases in cosmological parameters. Recently, forward modeling approaches to calibrate shear measurements within the Monte-Carlo Control Loops ($MCCL$) framework have been developed. These methods typically require simulating a large amount of wide-field images, thus, the simulations need to be very fast yet have realistic properties in key features such as the PSF pattern. Hence, such forward modeling approaches require a very flexible PSF model, which is quick to evaluate and whose parameters can be estimated reliably from survey data. We present a PSF model that meets these requirements based on a fast deep-learning method to estimate its free parameters. We demonstrate our approach on publicly available SDSS data. We extract the most important features of the SDSS sample via principal component analysis. Next, we construct our model based on perturbations of a fixed base profile, ensuring that it captures these features. We then train a Convolutional Neural Network to estimate the free parameters of the model from noisy images of the PSF. This allows us to render a model image of each star, which we compare to the SDSS stars to evaluate the performance of our method. We find that our approach is able to accurately reproduce the SDSS PSF at the pixel level, which, due to the speed of both the model evaluation and the parameter estimation, offers good prospects for incorporating our method into the $MCCL$ framework.}
}
@article{tortorelli1,
	title        = {Measurement of the B-band Galaxy Luminosity Function with Approximate Bayesian Computation},
	author       = {Tortorelli, Luca and Fagioli, Martina and Herbel, Jörg and Amara, Adam and Kacprzak, Tomasz and Refregier, Alexandre},
	year         = 2020,
	month        = jan,
	doi          = {10.1088/1475-7516/2020/09/048},
	url          = {https://arxiv.org/abs/2001.07727v2},
	abstractnote = {The galaxy Luminosity Function (LF) is a key observable for galaxy formation, evolution studies and for cosmology. In this work, we propose a novel technique to forward model wide-field broad-band galaxy surveys using the fast image simulator UFig and measure the LF of galaxies in the B-band. We use Approximate Bayesian Computation (ABC) to constrain the galaxy population model parameters of the simulations and match data from CFHTLS. We define a number of distance metrics between the simulated and the survey data. By exploring the parameter space of the galaxy population model through ABC to find the set of parameters that minimize these distance metrics, we obtain constraints on the LFs of blue and red galaxies as a function of redshift. We find that $mathrm{M^*}$ fades by $Delta mathrm{M}^*_{mathrm{0.1-1.0,b}} = 0.68 pm 0.52$ and $Delta mathrm{M}^*_{mathrm{0.1-1.0,r}} = 0.54 pm 0.48$ magnitudes between redshift $mathrm{z = 1}$ and $mathrm{z = 0.1}$ for blue and red galaxies, respectively. We also find that $phi^*$ for blue galaxies stays roughly constant between redshift $mathrm{z = 0.1}$ and $mathrm{z=1}$, while for red galaxies it decreases by $sim 35%$. We compare our results to other measurements, finding good agreement at all redshifts, for both blue and red galaxies. To further test our results, we compare the redshift distributions for survey and simulated data. We use the spectroscopic redshift distribution from the VIMOS Public Extragalactic Redshift Survey (VIPERS) and we apply the same selection in colours and magnitudes on our simulations. We find a good agreement between the survey and the simulated redshift distributions. We provide best-fit values and uncertainties for the parameters of the LF. This work offers excellent prospects for measuring other galaxy population properties as a function of redshift using ABC.},
	language     = {en}
}
@article{tortorelli2,
	title        = {The PAU Survey: Measurement of Narrow-band galaxy properties with Approximate Bayesian Computation},
	author       = {Tortorelli, Luca and Siudek, Malgorzata and Moser, Beatrice and Kacprzak, Tomasz and Berner, Pascale and Refregier, Alexandre and Amara, Adam and García-Bellido, Juan and Cabayol, Laura and Carretero, Jorge and Castander, Francisco J. and De Vicente, Juan and Eriksen, Martin and Fernandez, Enrique and Gaztanaga, Enrique and Hildebrandt, Hendrik and Joachimi, Benjamin and Miquel, Ramon and Sevilla-Noarbe, Ignacio and Padilla, Cristóbal and Renard, Pablo and Sanchez, Eusebio and Serrano, Santiago and Tallada-Crespí, Pau and Wright, Angus H.},
	year         = 2021,
	month        = jun,
	doi          = {10.1088/1475-7516/2021/12/013},
	url          = {https://arxiv.org/abs/2106.02651v2},
	abstractnote = {Narrow-band imaging surveys allow the study of the spectral characteristics of galaxies without the need of performing their spectroscopic follow-up. In this work, we forward-model the Physics of the Accelerating Universe Survey (PAUS) narrow-band data. The aim is to improve the constraints on the spectral coefficients used to create the galaxy spectral energy distributions (SED) of the galaxy population model in Tortorelli et al. 2020. In that work, the model parameters were inferred from the Canada-France-Hawaii Telescope Legacy Survey (CFHTLS) data using Approximate Bayesian Computation (ABC). This led to stringent constraints on the B-band galaxy luminosity function parameters, but left the spectral coefficients only broadly constrained. To address that, we perform an ABC inference using CFHTLS and PAUS data. This is the first time our approach combining forward-modelling and ABC is applied simultaneously to multiple datasets. We test the results of the ABC inference by comparing the narrow-band magnitudes of the observed and simulated galaxies using Principal Component Analysis, finding a very good agreement. Furthermore, we prove the scientific potential of the constrained galaxy population model to provide realistic stellar population properties by measuring them with the SED fitting code textsc{CIGALE}. We use CFHTLS broad-band and PAUS narrow-band photometry for a flux-limited ($mathrm{i}<22.5$) sample of galaxies up to redshift $mathrm{z sim 0.8}$. We find that properties like stellar masses, star-formation rates, mass-weighted stellar ages and metallicities are in agreement within errors between observations and simulations. Overall, this work shows the ability of our galaxy population model to correctly forward-model a complex dataset such as PAUS and the ability to reproduce the diversity of galaxy properties at the redshift range spanned by CFHTLS and PAUS.},
	language     = {en}
}
@article{kacprzak,
	title        = {Monte Carlo Control Loops for cosmic shear cosmology with DES Year 1},
	author       = {Kacprzak, T. and Herbel, J. and Nicola, A. and Sgier, R. and Tarsitano, F. and Bruderer, C. and Amara, A. and Refregier, A. and Bridle, S. L. and Drlica-Wagner, A. and Gruen, D. and Hartley, W. G. and Hoyle, B. and Secco, L. F. and Zuntz, J. and Annis, J. and Avila, S. and Bertin, E. and Brooks, D. and Buckley-Geer, E. and Rosell, A. Carnero and Kind, M. Carrasco and Carretero, J. and da Costa, L. N. and De Vicente, J. and Desai, S. and Diehl, H. T. and Doel, P. and García-Bellido, J. and Gaztanaga, E. and Gruendl, R. A. and Gschwend, J. and Gutierrez, G. and Hollowood, D. L. and Honscheid, K. and James, D. J. and Jarvis, M. and Lima, M. and Maia, M. A. G. and Marshall, J. L. and Melchior, P. and Menanteau, F. and Miquel, R. and Paz-Chinchón, F. and Plazas, A. A. and Sanchez, E. and Scarpine, V. and Serrano, S. and Sevilla-Noarbe, I. and Smith, M. and Suchyta, E. and Swanson, M. E. C. and Tarle, G. and Vikram, V. and Weller, J.},
	year         = 2020,
	month        = apr,
	journal      = {Physical Review D},
	volume       = 101,
	number       = 8,
	pages        = {082003},
	doi          = {10.1103/PhysRevD.101.082003},
	issn         = {2470-0010, 2470-0029},
	note         = {arXiv: 1906.01018},
	abstractnote = {Weak lensing by large-scale structure is a powerful probe of cosmology and of the dark universe. This cosmic shear technique relies on the accurate measurement of the shapes and redshifts of background galaxies and requires precise control of systematic errors. The Monte Carlo Control Loops (MCCL) is a forward modelling method designed to tackle this problem. It relies on the Ultra Fast Image Generator (UFig) to produce simulated images tuned to match the target data statistically, followed by calibrations and tolerance loops. We present the first end-to-end application of this method, on the Dark Energy Survey (DES) Year 1 wide field imaging data. We simultaneously measure the shear power spectrum $C_{ell}$ and the redshift distribution $n(z)$ of the background galaxy sample. The method includes maps of the systematic sources, Point Spread Function (PSF), an Approximate Bayesian Computation (ABC) inference of the simulation model parameters, a shear calibration scheme, and the fast estimation of the covariance matrix. We find a close statistical agreement between the simulations and the DES Y1 data using an array of diagnostics. In a non-tomographic setting, we derive a set of $C_ell$ and $n(z)$ curves that encode the cosmic shear measurement, as well as the systematic uncertainty. Following a blinding scheme, we measure the combination of $Omega_m$, $sigma_8$, and intrinsic alignment amplitude $A_{rm{IA}}$, defined as $S_8D_{rm{IA}} = sigma_8(Omega_m/0.3)^{0.5}D_{rm{IA}}$, where $D_{rm{IA}}=1-0.11(A_{rm{IA}}-1)$. We find $S_8D_{rm{IA}}=0.895^{+0.054}_{-0.039}$, where systematics are at the level of roughly 60% of the statistical errors. We discuss these results in the context of earlier cosmic shear analyses of the DES Y1 data. Our findings indicate that this method and its fast runtime offer good prospects for cosmic shear measurements with future wide-field surveys.},
	language     = {en}
}
@article{moser,
	title        = {Simulation-based inference of deep fields: galaxy population model and redshift distributions},
	author       = {Moser, Beatrice and Kacprzak, Tomasz and Fischbacher, Silvan and Refregier, Alexandre and Grimm, Dominic and Tortorelli, Luca},
	year         = 2024,
	month        = may,
	journal      = {Journal of Cosmology and Astroparticle Physics},
	volume       = 2024,
	number       = {05},
	pages        = {049},
	doi          = {10.1088/1475-7516/2024/05/049},
	issn         = {1475-7516},
	note         = {arXiv:2401.06846 [astro-ph]},
	abstractnote = {Accurate redshift calibration is required to obtain unbiased cosmological information from large-scale galaxy surveys. In a forward modelling approach, the redshift distribution n(z) of a galaxy sample is measured using a parametric galaxy population model constrained by observations. We use a model that captures the redshift evolution of the galaxy luminosity functions, colours, and morphology, for red and blue samples. We constrain this model via simulation-based inference, using factorized Approximate Bayesian Computation (ABC) at the image level. We apply this framework to HSC deep field images, complemented with photometric redshifts from COSMOS2020. The simulated telescope images include realistic observational and instrumental effects. By applying the same processing and selection to real data and simulations, we obtain a sample of n(z) distributions from the ABC posterior. The photometric properties of the simulated galaxies are in good agreement with those from the real data, including magnitude, colour and redshift joint distributions. We compare the posterior n(z) from our simulations to the COSMOS2020 redshift distributions obtained via template fitting photometric data spanning the wavelength range from UV to IR. We mitigate sample variance in COSMOS by applying a reweighting technique. We thus obtain a good agreement between the simulated and observed redshift distributions, with a difference in the mean at the 1$sigma$ level up to a magnitude of 24 in the i band. We discuss how our forward model can be applied to current and future surveys and be further extended. The ABC posterior and further material will be made publicly available at https://cosmology.ethz.ch/research/software-lab/ufig.html.}
}
@article{scikit-learn,
	title        = {Scikit-learn: Machine Learning in Python},
	author       = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Müller, Andreas and Nothman, Joel and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	year         = 2018,
	month        = jun,
	journal      = {arXiv:1201.0490 [cs]},
	url          = {http://arxiv.org/abs/1201.0490},
	note         = {arXiv: 1201.0490},
	abstractnote = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.}
}
@article{numpy,
	title        = {The NumPy array: a structure for efficient numerical computation},
	author       = {Van Der Walt, Stefan and Colbert, S. Chris and Varoquaux, Gaël},
	year         = 2011,
	month        = mar,
	journal      = {Computing in Science & Engineering},
	volume       = 13,
	number       = 2,
	pages        = {22–30},
	doi          = {10.1109/MCSE.2011.37},
	issn         = {1521-9615},
	note         = {arXiv: 1102.1523},
	abstractnote = {In the Python world, NumPy arrays are the standard representation for numerical data. Here, we show how these arrays enable efficient implementation of numerical computations in a high-level language. Overall, three techniques are applied to improve performance: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts. We first present the NumPy array structure, then show how to use it for efficient computation, and finally how to share array data with other libraries.}
}
@article{scipy,
	title        = {SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python},
	author       = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and Contributors, SciPy 1 0},
	year         = 2020,
	month        = mar,
	journal      = {Nature Methods},
	volume       = 17,
	number       = 3,
	pages        = {261–272},
	doi          = {10.1038/s41592-019-0686-2},
	issn         = {1548-7091, 1548-7105},
	note         = {arXiv: 1907.10121},
	abstractnote = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.}
}
@article{astropy,
	title        = {Astropy: A community Python package for astronomy},
	author       = {{Astropy Collaboration} and Robitaille, Thomas P. and Tollerud, Erik J. and Greenfield, Perry and Droettboom, Michael and Bray, Erik and Aldcroft, Tom and Davis, Matt and Ginsburg, Adam and Price-Whelan, Adrian M. and Kerzendorf, Wolfgang E. and Conley, Alexander and Crighton, Neil and Barbary, Kyle and Muna, Demitri and Ferguson, Henry and Grollier, Frédéric and Parikh, Madhura M. and Nair, Prasanth H. and Unther, Hans M. and Deil, Christoph and Woillez, Julien and Conseil, Simon and Kramer, Roban and Turner, James E. H. and Singer, Leo and Fox, Ryan and Weaver, Benjamin A. and Zabalza, Victor and Edwards, Zachary I. and Azalee Bostroem, K. and Burke, D. J. and Casey, Andrew R. and Crawford, Steven M. and Dencheva, Nadia and Ely, Justin and Jenness, Tim and Labrie, Kathleen and Lim, Pey Lian and Pierfederici, Francesco and Pontzen, Andrew and Ptak, Andy and Refsdal, Brian and Servillat, Mathieu and Streicher, Ole},
	year         = 2013,
	month        = oct,
	journal      = {Astronomy and Astrophysics},
	volume       = 558,
	pages        = {A33},
	doi          = {10.1051/0004-6361/201322068},
	issn         = {0004-6361},
	note         = {ADS Bibcode: 2013A&A...558A..33A},
	abstractnote = {We present the first public version (v0.2) of the open-source and community-developed Python package, Astropy. This package provides core astronomy-related functionality to the community, including support for domain-specific file formats such as flexible image transport system (FITS) files, Virtual Observatory (VO) tables, and common ASCII table formats, unit and physical quantity conversions, physical constants specific to astronomy, celestial coordinate and time transformations, world coordinate system (WCS) support, generalized containers for representing gridded as well as tabular data, and a framework for cosmological transformations and conversions. Significant functionality is under activedevelopment, such as a model fitting framework, VO client and server tools, and aperture and point spread function (PSF) photometry tools. The core development team is actively making additions and enhancements to the current code base, and we encourage anyone interested to participate in the development of future Astropy versions.}
}
@article{healpy,
	title        = {healpy: equal area pixelization and spherical harmonics transforms for data on the sphere in Python},
	author       = {Zonca, Andrea and Singer, Leo P. and Lenz, Daniel and Reinecke, Martin and Rosset, Cyrille and Hivon, Eric and Gorski, Krzysztof M.},
	year         = 2019,
	month        = mar,
	journal      = {Journal of Open Source Software},
	volume       = 4,
	number       = 35,
	pages        = 1298,
	doi          = {10.21105/joss.01298},
	issn         = {2475-9066},
	abstractnote = {Zonca et al., (2019). healpy: equal area pixelization and spherical harmonics transforms for data on the sphere in Python. Journal of Open Source Software, 4(35), 1298, https://doi.org/10.21105/joss.01298},
	language     = {en}
}
@article{sextractor,
	title        = {SExtractor: Software for source extraction},
	author       = {Bertin, E. and Arnouts, S.},
	year         = 1996,
	month        = jun,
	journal      = {Astronomy and Astrophysics Supplement Series},
	publisher    = {EDP Sciences},
	volume       = 117,
	number       = 22,
	pages        = {393–404},
	doi          = {10.1051/aas:1996164},
	issn         = {0365-0138, 1286-4846},
	rights       = {© European Southern Observatory (ESO), 1996},
	abstractnote = {We present the automated techniques we have developed for new software that optimally detects, deblends, measures and classifies sources from astronomical images: SExtractor (<i>Source Extractor <i/>). We show that a very reliable star/galaxy separation can be achieved on most images using a neural network trained with <i>simulated<i/> images. Salient features of SExtractor include its ability to work on very large images, with minimal human intervention, and to deal with a wide variety of object shapes and magnitudes. It is therefore particularly suited to the analysis of large extragalactic surveys.},
	language     = {en}
}
@article{kcorrect,
	title        = {K-corrections and filter transformations in the ultraviolet, optical, and near infrared},
	author       = {Blanton, Michael R. and Roweis, Sam},
	year         = 2007,
	month        = feb,
	journal      = {The Astronomical Journal},
	volume       = 133,
	number       = 2,
	pages        = {734–754},
	doi          = {10.1086/510127},
	issn         = {0004-6256, 1538-3881},
	note         = {arXiv:astro-ph/0606170},
	abstractnote = {Template fits to observed galaxy fluxes allow calculation of K-corrections and conversions among observations of galaxies at various wavelengths. We present a method for creating model-based template sets given a set of heterogeneous photometric and spectroscopic galaxy data. Our technique, non-negative matrix factorization, is akin to principle component analysis (PCA), except that it is constrained to produce nonnegative templates, it can use a basis set of models (rather than the delta function basis of PCA), and it naturally handles uncertainties, missing data, and heterogeneous data (including broad-band fluxes at various redshifts). The particular implementation we present here is suitable for ultraviolet, optical, and near-infrared observations in the redshift range 0 < z < 1.5. Since we base our templates on stellar population synthesis models, the results are intepretable in terms of approximate stellar masses and star-formation histories. We present templates fit with this method to data from GALEX, Sloan Digital Sky Survey spectroscopy and photometry, the Two-Micron All Sky Survey, the Deep Extragalactic Evolutionary Probe and the Great Observatories Origins Deep Survey. In addition, we present software for using such data to estimate K-corrections and stellar masses.}
}
@article{pycosmo1,
	title        = {PyCosmo: An integrated cosmological Boltzmann solver},
	author       = {Refregier, A. and Gamper, L. and Amara, A. and Heisenberg, L.},
	year         = 2018,
	month        = oct,
	journal      = {Astronomy and Computing},
	volume       = 25,
	pages        = {38–43},
	doi          = {10.1016/j.ascom.2018.08.001},
	issn         = {2213-1337},
	abstractnote = {As wide-field surveys yield ever more precise measurements, cosmology has entered a phase of high precision requiring highly accurate and fast theoretical predictions. At the heart of most cosmological model predictions is a numerical solution of the Einstein–Boltzmann equations governing the evolution of linear perturbations in the Universe. We present PyCosmo, a new Python-based framework to solve this set of equations using a special purpose solver based on symbolic manipulations, automatic generation of C++ code and sparsity optimisation. The code uses a consistency relation of the field equations to adapt the time step and does not rely on physical approximations for speed-up. After reviewing the system of first-order linear homogeneous differential equations to be solved, we describe the numerical scheme implemented in PyCosmo. We then compare the predictions and performance of the code for the computation of the transfer functions of cosmological perturbations and compare it to existing cosmological Boltzmann codes. While PyCosmo does not yet have all the features of other codes, our approach is complementary to other fast cosmological Boltzmann solvers and can be used as an independent test of their numerical solutions. The symbolic representation of the Einstein–Boltzmann equation system in PyCosmo provides a convenient interface for implementing extended cosmological models. We also discuss how the PyCosmo framework can also be used as a general framework to compute cosmological quantities as well as observables for both interactive and high-performance batch jobs applications. Information about the PyCosmo package and future code releases are available at http://www.cosmology.ethz.ch/research/software-lab.html.},
	language     = {en}
}
@article{pycosmo2,
	title        = {Predicting cosmological observables with PyCosmo},
	author       = {Tarsitano, F. and Schmitt, U. and Refregier, A. and Fluri, J. and Sgier, R. and Nicola, A. and Herbel, J. and Amara, A. and Kacprzak, T. and Heisenberg, L.},
	year         = 2021,
	journal      = {Astronomy and Computing},
	volume       = 36,
	pages        = 100484,
	doi          = {https://doi.org/10.1016/j.ascom.2021.100484},
	issn         = {2213-1337},
	abstractnote = {Current and upcoming cosmological experiments open a new era of precision cosmology, thus demanding accurate theoretical predictions for cosmological observables. Because of the complexity of the codes delivering such predictions, reaching a high level of numerical accuracy is challenging. Among the codes already fulfilling this task, PyCosmo is a Python-based framework providing solutions to the Einstein–Boltzmann equations and accurate predictions for cosmological observables. We present the first public release of the code, which is valid in ΛCDM cosmology. The novel aspect of this version is that the user can work within a Python framework, either locally or through an online platform, called PyCosmo Hub. In this work we first describe how the observables are implemented. Then, we check the accuracy of the theoretical predictions for background quantities, power spectra and Limber and beyond-Limber angular power spectra by comparison with other codes: the Core Cosmology Library (CCL), CLASS, HMCode and iCosmo. In our analysis we quantify the agreement of PyCosmo with the other codes, for a range of cosmological models, monitored through a series of unit tests. PyCosmo, conceived as a multi-purpose cosmology calculation tool in Python, is designed to be interactive and user-friendly. The PyCosmo Hub is accessible from this link: https://cosmology.ethz.ch/research/software-lab/PyCosmo.html. On this platform the users can perform their own computations using Jupyter Notebooks without the need of installing any software, access to the results presented in this work and benefit from tutorial notebooks illustrating the usage of the code. The link above also redirects to the code release and documentation.}
}
@article{pycosmo3,
	title        = {Symbolic implementation of extensions of the PyCosmo Boltzmann solver},
	author       = {Moser, B. and Lorenz, C. S. and Schmitt, U. and Réfrégier, A. and Fluri, J. and Sgier, R. and Tarsitano, F. and Heisenberg, L.},
	year         = 2022,
	month        = jul,
	journal      = {Astronomy and Computing},
	volume       = 40,
	pages        = 100603,
	doi          = {10.1016/j.ascom.2022.100603},
	issn         = {2213-1337},
	abstractnote = {PyCosmo is a Python-based framework for the fast computation of cosmological model predictions. One of its core features is the symbolic representation of the Einstein–Boltzmann system of equations. Efficient C/C++ code is generated from the SymPy symbolic expressions making use of the sympy2c package. This enables easy extensions of the equation system for the implementation of new cosmological models. We illustrate this with three extensions of the PyCosmo Boltzmann solver to include a dark energy component with a constant equation of state, massive neutrinos and a radiation streaming approximation. We describe the PyCosmo framework, highlighting new features, and the symbolic implementation of the new models. We compare the PyCosmo predictions for the ΛCDM model extensions with CLASS, both in terms of accuracy and computational speed. We find a good agreement, to better than 0.1% when using high-precision settings and a comparable computational speed. Links to the Python Package Index (PyPI) page of the code release and to the PyCosmo Hub, an online platform where the package is installed, are available at: https://cosmology.ethz.ch/research/software-lab/PyCosmo.html.}
}
@article{matplotlib,
	title        = {Matplotlib: A 2D Graphics Environment},
	author       = {Hunter, John D.},
	year         = 2007,
	month        = may,
	journal      = {Computing in Science Engineering},
	volume       = 9,
	number       = 3,
	pages        = {90–95},
	doi          = {10.1109/MCSE.2007.55},
	issn         = {1558-366X},
	abstractnote = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems}
}
@online{plotly,
	title        = {Collaborative data science},
	author       = {{Plotly Technologies Inc.}},
	year         = 2015,
	publisher    = {Plotly Technologies Inc.},
	address      = {Montreal, QC},
	url          = {https://plot.ly}
}
@article{trianglechain1,
	title        = {Redshift requirements for cosmic shear with intrinsic alignment},
	author       = {Fischbacher, Silvan and Kacprzak, Tomasz and Blazek, Jonathan and Refregier, Alexandre},
	year         = 2023,
	month        = jan,
	journal      = {Journal of Cosmology and Astroparticle Physics},
	volume       = 2023,
	number       = {01},
	pages        = {033},
	doi          = {10.1088/1475-7516/2023/01/033},
	issn         = {1475-7516},
	note         = {arXiv:2207.01627 [astro-ph]},
	abstractnote = {Intrinsic alignment (IA) modelling and photometric redshift estimation are two of the main sources of systematic uncertainty in weak lensing surveys. We investigate the impact of redshift errors and their interplay with different IA models. Generally, errors on the mean $delta_z$ and on the width $sigma_z$ of the redshift bins can both lead to biases in cosmological constraints. We find that such biases can, however, only be partially resolved by marginalizing over $delta_z$ and $sigma_z$. For Stage-III surveys, $delta_z$ and $sigma_z$ cannot be well constrained due to limited statistics. The resulting biases are thus sensitive to prior volume effects. For Stage-IV surveys, we observe that marginalizing over the redshift parameters has an impact and reduces the bias. We derive requirements on the uncertainty of $sigma_z$ and $delta_z$ for both Stage-III and Stage-IV surveys. We assume that the redshift systematic errors on $S_8$ should be less than half of the statistical errors, and the median bias should be smaller than $0.25sigma$. We find that the uncertainty on $delta_z$ has to be $lesssim0.025$ for the NLA IA model with a Stage-III survey. For $sigma_z$, the requirement is met even for large uncertainties $leq0.3$. For the TATT IA model, the uncertainty on $delta_z$ has to be $lesssim0.02$ and the uncertainty on $sigma_z$ has to be $lesssim0.2$. For Stage-IV surveys, the uncertainty on $delta_z$ has to be $lesssim0.005$ and the uncertainty on $sigma_z$ should be $lesssim0.1$, with no significant dependence on the IA model. This required high precision will be a challenge for the redshift calibration of these future surveys. Finally, we investigate whether the interplay between redshift systematics and IA modelling can explain the $S_8$-tension between cosmic shear results and CMB measurements. We find that this is unlikely to explain the current $S_8$-tension.}
}
@article{trianglechain2,
	title        = {DeepLSS: Breaking Parameter Degeneracies in Large-Scale Structure with Deep-Learning Analysis of Combined Probes},
	author       = {Kacprzak, Tomasz and Fluri, Janis},
	year         = 2022,
	month        = jul,
	journal      = {Physical Review X},
	volume       = 12,
	pages        = {031029},
	doi          = {10.1103/PhysRevX.12.031029},
	note         = {arXiv:2203.09616 [astro-ph]},
	abstractnote = {In classical cosmological analysis of large-scale structure surveys with two-point functions, the parameter measurement precision is limited by several key degeneracies within the cosmology and astrophysics sectors. For cosmic shear, clustering amplitude σ8 and matter density Ωm roughly follow the S8=σ8(Ωm/0.3 )0.5 relation. In turn, S8 is highly correlated with the intrinsic galaxy alignment amplitude AIA. For galaxy clustering, the bias bg is degenerate with both σ8 and Ωm, as well as the stochasticity rg. Moreover, the redshift evolution of intrinsic alignment (IA) and bias can cause further parameter confusion. A tomographic two-point probe combination can partially lift these degeneracies. In this work we demonstrate that a deep-learning analysis of combined probes of weak gravitational lensing and galaxy clustering, which we call DeepLSS, can effectively break these degeneracies and yield significantly more precise constraints on σ8, Ωm, AIA, bg, rg, and IA redshift evolution parameter ηIA. In a simulated forecast for a stage-III survey, we find that the most significant gains are in the IA sector: the precision of AIA is increased by approximately 8 times and is almost perfectly decorrelated from S8. Galaxy bias bg is improved by 1.5 times, stochasticity rg by 3 times, and the redshift evolution ηIA and ηb by 1.6 times. Breaking these degeneracies leads to a significant gain in constraining power for σ8 and Ωm, with the figure of merit improved by 15 times. We give an intuitive explanation for the origin of this information gain using sensitivity maps. These results indicate that the fully numerical, map-based forward-modeling approach to cosmological inference with machine learning may play an important role in upcoming large-scale structure surveys. We discuss perspectives and challenges in its practical deployment for a full survey analysis.}
}
@article{sbi_review,
	title        = {The frontier of simulation-based inference},
	author       = {Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
	year         = 2020,
	month        = dec,
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {Proceedings of the National Academy of Sciences},
	volume       = 117,
	number       = 48,
	pages        = {30055–30062},
	doi          = {10.1073/pnas.1912789117},
	abstractnote = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving additional momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound influence these developments may have on science.}
}
@unpublished{galsbi_sps,
  author       = {Luca Tortorelli and others},
  title        = {GalSBI: Galaxy population model with SPS},
  note         = {Manuscript in preparation},
  year         = {2025},
}

@article{fagioli1,
	title        = {Forward modeling of spectroscopic galaxy surveys: application to SDSS},
	author       = {Fagioli, Martina and Riebartsch, Julian and Nicola, Andrina and Herbel, Jörg and Amara, Adam and Refregier, Alexandre and Chang, Chihway and Gamper, Laurenz and Tortorelli, Luca},
	year         = 2018,
	month        = nov,
	journal      = {Journal of Cosmology and Astroparticle Physics},
	volume       = 2018,
	number       = 11,
	pages        = {015},
	doi          = {10.1088/1475-7516/2018/11/015},
	issn         = {1475-7516},
	abstractnote = {Galaxy spectra are essential to probe the spatial distribution of galaxies in our Universe. To better interpret current and future spectroscopic galaxy redshift surveys, it is important to be able to simulate these data sets. We describe USPEC, a forward modeling tool to generate galaxy spectra taking into account some intrinsic galaxy properties as well as instrumental responses of a given telescope. The model for the intrinsic properties of the galaxy population, i.e., the luminosity functions, and size and spectral coefficients distributions, was developed in an earlier work for broad-band imaging surveys [1], and we now aim to test the model further using spectroscopic data. We apply USPEC to the SDSS/CMASS sample of Luminous Red Galaxies (LRGs). We construct selection cuts that match those used to build this LRG sample, which we then apply to data and simulations in the same way. The resulting real and simulated average spectra show a good statistical agreement overall, with residual differences likely coming from a bluer galaxy population of the simulated sample. We also do not explore the impact of non-solar element ratios in our simulations. For a quantitative comparison, we perform Principal Component Analysis (PCA) of the sets of spectra. By comparing the PCs constructed from simulations and data, we find good agreement for all components. The distributions of the eigencoefficients also show an appreciable overlap. We are therefore able to properly simulate the LRG sample taking into account the SDSS/BOSS instrumental responses. The differences between the two samples can be ascribed to the intrinsic properties of the simulated galaxy population, which can be reduced by further improvements of our modeling method in the future. We discuss how these results can be useful for the forward modeling of upcoming large spectroscopic surveys.},
	language     = {en}
}
@article{fagioli2,
	title        = {Spectro-imaging forward model of red and blue galaxies},
	author       = {Fagioli, Martina and Tortorelli, Luca and Herbel, Jörg and Zürcher, Dominik and Refregier, Alexandre and Amara, Adam},
	year         = 2020,
	month        = jun,
	journal      = {Journal of Cosmology and Astroparticle Physics},
	volume       = 2020,
	number       = {06},
	pages        = {050},
	doi          = {10.1088/1475-7516/2020/06/050},
	issn         = {1475-7516},
	abstractnote = {For the next generation of spectroscopic galaxy surveys, it is important to forecast their performances and to accurately interpret their large data sets. For this purpose, it is necessary to consistently simulate different populations of galaxies, in particular Emission Line Galaxies (ELGs), less used in the past for cosmological purposes. In this work, we further the forward modeling approach presented in Fagioli et al. 2018, by extending the spectra simulator USPEC to model galaxies of different kinds with improved parameters from Tortorelli et al. 2020. Furthermore, we improve the modeling of the selection function by using the image simulator UFIG. We apply this to the Sloan Digital Sky Survey (SDSS), and simulate ∼157,000 multi-band images. We pre-process and analyse them to apply cuts for target selection, and finally simulate SDSS/BOSS DR14 galaxy spectra. We compute photometric, astrometric and spectroscopic properties for red and blue, real and simulated galaxies, finding very good agreement. We compare the statistical properties of the samples by decomposing them with Principal Component Analysis (PCA). We find very good agreement for red galaxies and a good, but less pronounced one, for blue galaxies, as expected given the known difficulty of simulating those. Finally, we derive stellar population properties, mass-to-light ratios, ages and metallicities, for all samples, finding again very good agreement. This shows how this method can be used not only to forecast cosmology surveys, but it is also able to provide insights into studies of galaxy formation and evolution.},
	language     = {en}
}
@article{berner,
	title        = {Fast forward modelling of galaxy spatial and statistical distributions},
	author       = {Berner, Pascale and Refregier, Alexandre and Moser, Beatrice and Tortorelli, Luca and Machado Poletti Valle, Luis Fernando and Kacprzak, Tomasz},
	year         = 2024,
	month        = apr,
	journal      = {Journal of Cosmology and Astroparticle Physics},
	volume       = 2024,
	number       = {04},
	pages        = {023},
	doi          = {10.1088/1475-7516/2024/04/023},
	issn         = {1475-7516},
	note         = {arXiv:2310.15223 [astro-ph]},
	abstractnote = {
		Abstract

		A forward modelling approach provides simple, fast and realistic simulations of galaxy surveys, without a complex underlying model. For this purpose, galaxy clustering needs to be simulated accurately, both for the usage of clustering as its own probe and to control systematics. We present a forward model to simulate galaxy surveys, where we extend the Ultra-Fast Image Generator to include galaxy clustering. We use the distribution functions of the galaxy properties, derived from a forward model adjusted to observations. This population model jointly describes the luminosity functions, sizes, ellipticities, SEDs and apparent magnitudes. To simulate the positions of galaxies, we then use a two-parameter relation between galaxies and halos with Subhalo Abundance Matching (SHAM). We simulate the halos and subhalos using the fast PINOCCHIO code, and a method to extract the surviving subhalos from the merger history. Our simulations contain a red and a blue galaxy population, for which we build a SHAM model based on star formation quenching. For central galaxies, mass quenching is controlled with the parameter M limit , with blue galaxies residing in smaller halos. For satellite galaxies, environmental quenching is implemented with the parameter t quench , where blue galaxies occupy only recently merged subhalos. We build and test our model by comparing to imaging data from the Dark Energy Survey Year 1. To ensure completeness in our simulations, we consider the brightest galaxies with i < 20. We find statistical agreement between our simulations and the data for two-point correlation functions on medium to large scales. Our model provides constraints on the two SHAM parameters M limit and t quench and offers great prospects for the quick generation of galaxy mock catalogues, optimized to agree with observations.
	}
}

@article{bruderer1,
	title        = {Calibrated Ultra Fast Image Simulations for the Dark Energy Survey},
	author       = {
		Bruderer, Claudio and Chang, Chihway and Refregier, Alexandre and Amara, Adam
		and Berge, Joel and Gamper, Lukas
	},
	year         = 2016,
	month        = jan,
	journal      = {The Astrophysical Journal},
	volume       = 817,
	number       = 1,
	pages        = 25,
	doi          = {10.3847/0004-637X/817/1/25},
	issn         = {0004-637X, 1538-4357},
	note         = {arXiv:1504.02778 [astro-ph]},
	abstractnote = {
		Weak lensing by large-scale structure is a powerful technique to probe the
		dark components of the universe. To understand the measurement process of
		weak lensing and the associated systematic effects, image simulations are
		becoming increasingly important. For this purpose we present a first
		implementation of the $textit{Monte Carlo Control Loops}$ ($textit{MCCL}$;
		Refregier & Amara 2014), a coherent framework for studying systematic effects
		in weak lensing. It allows us to model and calibrate the shear measurement
		process using image simulations from the Ultra Fast Image Generator (UFig;
		Berge et al. 2013). We apply this framework to a subset of the data taken
		during the Science Verification period (SV) of the Dark Energy Survey (DES).
		We calibrate the UFig simulations to be statistically consistent with DES
		images. We then perform tolerance analyses by perturbing the simulation
		parameters and study their impact on the shear measurement at the one-point
		level. This allows us to determine the relative importance of different input
		parameters to the simulations. For spatially constant systematic errors and
		six simulation parameters, the calibration of the simulation reaches the weak
		lensing precision needed for the DES SV survey area. Furthermore, we find a
		sensitivity of the shear measurement to the intrinsic ellipticity
		distribution, and an interplay between the magnitude-size and the pixel value
		diagnostics in constraining the noise model. This work is the first
		application of the $textit{MCCL}$ framework to data and shows how it can be
		used to methodically study the impact of systematics on the cosmic shear
		measurement.
	}
}
@article{bruderer2,
	title        = {Cosmic shear calibration with forward modeling},
	author       = {
		Bruderer, Claudio and Nicola, Andrina and Amara, Adam and Refregier,
		Alexandre and Herbel, Jörg and Kacprzak, Tomasz
	},
	year         = 2018,
	month        = aug,
	journal      = {Journal of Cosmology and Astroparticle Physics},
	volume       = 2018,
	number       = {08},
	pages        = {007},
	doi          = {10.1088/1475-7516/2018/08/007},
	issn         = {1475-7516},
	abstractnote = {
		Weak Gravitational Lensing is a powerful probe of the dark sector of the
		Universe. One of the main challenges for this technique is the treatment of
		systematics in the measurement of cosmic shear from galaxy shapes. In an
		earlier work, [1] have proposed the Monte Carlo Control Loops (MCCL) to
		overcome these effects using a forward modeling approach. We focus here on
		one of the control loops in this method, the task of which is the calibration
		of the shear measurement. For this purpose, we first consider the
		requirements on the shear systematics for a given survey and propagate them
		to different systematics terms. We use two one-point statistics to calibrate
		the shear measurement and six further one-point statistics as diagnostics. We
		also propagate the systematics levels that we estimate from the one-point
		functions to the two-point functions for the different systematic error
		sources. This allows us to assess the consistency between the systematics
		levels measured in different ways. To test the method, we construct synthetic
		sky surveys with an area of 1,700°2. With some simplifying assumptions, we
		are able to meet the requirements on the shear calibration for this survey
		configuration. Furthermore, we account for the total residual shear
		systematics in terms of the contributing sources. We discuss how this
		framework can be applied within analyses of current and future weak lensing
		surveys.
	},
	language     = {en}
}
@article{chang,
	title        = {Modeling the Transfer Function for the Dark Energy Survey},
	author       = {
		Chang, C. and Busha, M. T. and Wechsler, R. H. and Refregier, A. and Amara,
		A. and Rykoff, E. and Becker, M. R. and Bruderer, C. and Gamper, L. and
		Leistedt, B. and Peiris, H. and Abbott, T. and Abdalla, F. B. and Balbinot,
		E. and Banerji, M. and Bernstein, R. A. and Bertin, E. and Brooks, D. and
		Carnero, A. and Desai, S. and da Costa, L. N. and Cunha, C. E. and Eifler, T.
		and Evrard, A. E. and Fausti Neto, A. and Gerdes, D. and Gruen, D. and James,
		D. and Kuehn, K. and Maia, M. A. G. and Makler, M. and Ogando, R. and Plazas,
		A. and Sanchez, E. and Santiago, B. and Schubnell, M. and Sevilla-Noarbe, I.
		and Smith, C. and Soares-Santos, M. and Suchyta, E. and Swanson, M. E. C. and
		Tarle, G. and Zuntz, J.
	},
	year         = 2015,
	month        = mar,
	journal      = {The Astrophysical Journal},
	publisher    = {IOP},
	volume       = 801,
	pages        = 73,
	doi          = {10.1088/0004-637X/801/2/73},
	issn         = {0004-637X},
	note         = {ADS Bibcode: 2015ApJ...801...73C},
	abstractnote = {
		We present a forward-modeling simulation framework designed to model the data
		products from the Dark Energy Survey (DES). This forward-model process can be
		thought of as a transfer function—a mapping from cosmological/astronomical
		signals to the final data products used by the scientists. Using output from
		the cosmological simulations (the Blind Cosmology Challenge), we generate
		simulated images (the Ultra Fast Image Simulator) and catalogs representative
		of the DES data. In this work we demonstrate the framework by simulating the
		244 deg2 coadd images and catalogs in five bands for the DES Science
		Verification data. The simulation output is compared with the corresponding
		data to show that major characteristics of the images and catalogs can be
		captured. We also point out several directions of future improvements. Two
		practical examples—star-galaxy classification and proximity effects on object
		detection—are then used to illustrate how one can use the simulations to
		address systematics issues in data analysis. With clear understanding of the
		simplifications in our model, we show that one can use the simulations
		side-by-side with data products to interpret the measurements. This forward
		modeling approach is generally applicable for other upcoming and future
		surveys. It provides a powerful tool for systematics studies that is
		sufficiently realistic and highly controllable.
	}
}
