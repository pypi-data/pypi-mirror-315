Metadata-Version: 2.1
Name: mloptiflow
Version: 0.0.12
Summary: ML Lifecycle Management Framework
Home-page: https://gitlab.com/mloptiflow/mloptiflow
License: MIT
Author: Tomas Kozak
Author-email: tomas.kozak.inv@gmail.com
Requires-Python: >=3.11,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Dist: PyYAML (>=6.0.2,<7.0.0)
Requires-Dist: click (>=8.1.7,<9.0.0)
Requires-Dist: fastapi (>=0.115.5,<0.116.0)
Requires-Dist: python-dotenv (>=1.0.0,<2.0.0)
Requires-Dist: uvicorn (>=0.32.1,<0.33.0)
Project-URL: Repository, https://gitlab.com/mloptiflow/mloptiflow
Description-Content-Type: text/markdown

# MLOPTIFLOW

Python framework and library with an integrated CLI, designed to streamline ML lifecycle management by automating project inception, real-time model monitoring, and adaptive canary deployment processes. The library facilitates automated project setup across various configurable ML paradigms (whether it is a Random Forest classification, XGBoost regression, T5-based time-series forecasting, encoder-only / encoder-decoder / decoder-only transformer-based NLP downstream tasks, or practically any other configurable ML paradigm) with optimized directories, subdirectories and configuration files adhering to recommended practices in ML development. The library features a plugin architecture for extensibility, allowing integration with other core components / packages, such as real-time model monitoring with anomaly detection mechanisms, adaptive canary deployment architectures, and integrated UI for monitoring and deployment control. Model monitoring is implemented using high-throughput, low-latency data streaming tool Apache Kafka. Deployed ML models act as Kafka producers, emitting real-time inference data and performance metrics serialized with Apache Avro for schema enforcement and efficiency. Model monitoring is accompanied with anomaly, data, and concept drift detection mechanisms via techniques like PSI, Isolation Forests, or LSTM auto-encoders. Adaptive canary deployment architectures and strategies are implemented specifically for ML models using Kubernetes for container orchestration and Istio as a service mesh for traffic management and routing between baseline and canary versions at granular levels. Integrated UI for monitoring and deployment control is implemented using Vanilla JavaScript and Bootstrap on the client-side, and FastAPI / LitServe on the server-side.


## Installation

1. create a new virtual environment with python ^3.11 and activate it

2. install mloptiflow:

```bash
pip install mloptiflow
```

3. initialize a new project and choose a name and paradigm (currently supported paradigms are: `tabular_regression`, `tabular_classification`):

```bash
mloptiflow init <your-project-name> --paradigm=<paradigm-name>
```

4. `cd` into your project directory and (if using `poetry`) update `name` field in `pyproject.toml` file:

```bash
cd <your-project-name>
```

```toml
[tool.poetry]
name = "<your-project-name>"
```

5. optionally, create a root package for the project and add `__init__.py` file:

```bash
mkdir <your-project-name>
touch <your-project-name>/__init__.py
```

6. install dependencies:

```bash
poetry install --no-root
```

or (if you created root package):

```bash
poetry install
```

or if using `pip`:

```bash
pip install -r requirements.txt
```

## Usage
1. run the application:

```bash
streamlit run app.py
```

or:

```bash
poetry run streamlit run app.py
```

2. optionally, adjust `Dockerfile` to your needs if you want to run the inference application in a containerized environment:

```dockerfile
# mainly the WORKDIR
WORKDIR /<your-project-name>
```

3. build the container image:

```bash
docker build -t <your-project-name> .
```

4. run the container image:

```bash
docker run -p 8501:8501 <your-project-name>
```

## Support
- TBA

## Roadmap
- TBA

## Contributing
- TBA


## License
MIT

