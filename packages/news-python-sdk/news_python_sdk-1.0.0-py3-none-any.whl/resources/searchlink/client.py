# This file was auto-generated by Fern from our API Definition.

import typing
import urllib.parse
from json.decoder import JSONDecodeError

from ...core.api_error import ApiError
from ...core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ...core.jsonable_encoder import jsonable_encoder
from ...core.remove_none_from_dict import remove_none_from_dict
from ...errors.bad_request_error import BadRequestError
from ...errors.forbidden_error import ForbiddenError
from ...errors.internal_server_error import InternalServerError
from ...errors.request_timeout_error import RequestTimeoutError
from ...errors.too_many_requests_error import TooManyRequestsError
from ...errors.unauthorized_error import UnauthorizedError
from ...errors.unprocessable_entity_error import UnprocessableEntityError
from ...types.dto_responses_search_response_search_response import DtoResponsesSearchResponseSearchResponse
from ...types.error import Error
from .types.search_link_post_request import SearchLinkPostRequest

try:
    import pydantic.v1 as pydantic  # type: ignore
except ImportError:
    import pydantic  # type: ignore

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class SearchlinkClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def get(
        self,
        *,
        ids: typing.Optional[str] = None,
        links: typing.Optional[str] = None,
        from_: typing.Optional[str] = None,
        to: typing.Optional[str] = None,
        page: typing.Optional[int] = None,
        page_size: typing.Optional[int] = None,
    ) -> DtoResponsesSearchResponseSearchResponse:
        """
        Searches for articles based on specified links or IDs. You can filter results by date range.

        Parameters:
            - ids: typing.Optional[str].

            - links: typing.Optional[str].

            - from_: typing.Optional[str].

            - to: typing.Optional[str].

            - page: typing.Optional[int]. The page number to scroll through the results.
                                          This parameter is used to paginate: scroll through results because one API response cannot return more than 1000 articles.
            - page_size: typing.Optional[int]. The number of articles to return per page.
                                               Range: `1` to `1000`.---
        from newscatcher.client import NewscatcherApi

        client = NewscatcherApi(
            api_key="YOUR_API_KEY",
        )
        client.searchlink.get()
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/search_by_link"),
            params=remove_none_from_dict(
                {"ids": ids, "links": links, "from_": from_, "to_": to, "page": page, "page_size": page_size}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DtoResponsesSearchResponseSearchResponse, _response.json())  # type: ignore
        if _response.status_code == 400:
            raise BadRequestError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 401:
            raise UnauthorizedError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 403:
            raise ForbiddenError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 408:
            raise RequestTimeoutError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 429:
            raise TooManyRequestsError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 500:
            raise InternalServerError(pydantic.parse_obj_as(str, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def post(self, *, request: SearchLinkPostRequest) -> DtoResponsesSearchResponseSearchResponse:
        """
        Searches for articles using their ID(s) or link(s).

        Parameters:
            - request: SearchLinkPostRequest.
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/search_by_link"),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DtoResponsesSearchResponseSearchResponse, _response.json())  # type: ignore
        if _response.status_code == 400:
            raise BadRequestError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 401:
            raise UnauthorizedError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 403:
            raise ForbiddenError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 408:
            raise RequestTimeoutError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 429:
            raise TooManyRequestsError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 500:
            raise InternalServerError(pydantic.parse_obj_as(str, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncSearchlinkClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def get(
        self,
        *,
        ids: typing.Optional[str] = None,
        links: typing.Optional[str] = None,
        from_: typing.Optional[str] = None,
        to: typing.Optional[str] = None,
        page: typing.Optional[int] = None,
        page_size: typing.Optional[int] = None,
    ) -> DtoResponsesSearchResponseSearchResponse:
        """
        Searches for articles based on specified links or IDs. You can filter results by date range.

        Parameters:
            - ids: typing.Optional[str].

            - links: typing.Optional[str].

            - from_: typing.Optional[str].

            - to: typing.Optional[str].

            - page: typing.Optional[int]. The page number to scroll through the results.
                                          This parameter is used to paginate: scroll through results because one API response cannot return more than 1000 articles.
            - page_size: typing.Optional[int]. The number of articles to return per page.
                                               Range: `1` to `1000`.---
        from newscatcher.client import AsyncNewscatcherApi

        client = AsyncNewscatcherApi(
            api_key="YOUR_API_KEY",
        )
        await client.searchlink.get()
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/search_by_link"),
            params=remove_none_from_dict(
                {"ids": ids, "links": links, "from_": from_, "to_": to, "page": page, "page_size": page_size}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DtoResponsesSearchResponseSearchResponse, _response.json())  # type: ignore
        if _response.status_code == 400:
            raise BadRequestError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 401:
            raise UnauthorizedError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 403:
            raise ForbiddenError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 408:
            raise RequestTimeoutError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 429:
            raise TooManyRequestsError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 500:
            raise InternalServerError(pydantic.parse_obj_as(str, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def post(self, *, request: SearchLinkPostRequest) -> DtoResponsesSearchResponseSearchResponse:
        """
        Searches for articles using their ID(s) or link(s).

        Parameters:
            - request: SearchLinkPostRequest.
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/search_by_link"),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DtoResponsesSearchResponseSearchResponse, _response.json())  # type: ignore
        if _response.status_code == 400:
            raise BadRequestError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 401:
            raise UnauthorizedError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 403:
            raise ForbiddenError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 408:
            raise RequestTimeoutError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 429:
            raise TooManyRequestsError(pydantic.parse_obj_as(Error, _response.json()))  # type: ignore
        if _response.status_code == 500:
            raise InternalServerError(pydantic.parse_obj_as(str, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
