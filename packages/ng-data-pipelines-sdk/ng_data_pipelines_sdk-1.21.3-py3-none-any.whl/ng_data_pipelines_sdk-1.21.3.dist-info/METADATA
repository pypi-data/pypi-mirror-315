Metadata-Version: 2.1
Name: ng-data-pipelines-sdk
Version: 1.21.3
Summary: A library for interacting with data from Amazon S3 through PySpark. Read, write and transform data using a powerful and intuitive API with strong consistency and type checking, thanks to Pydantic. Compatible with Amazon MWAA running Airflow 2.7.2 and above.
Author: ng.cash
Requires-Python: >=3.9,<3.12
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Dist: boto3 (>=1.28.17,<2.0.0)
Requires-Dist: boto3-stubs[s3,secretsmanager] (>=1.35.52,<2.0.0)
Requires-Dist: load-dotenv (>=0.1.0,<0.2.0)
Requires-Dist: pandas (>=2.1.1,<3.0.0)
Requires-Dist: pydantic (>=2.4.2,<3.0.0)
Requires-Dist: pydantic-settings (>=2.6.1,<3.0.0)
Requires-Dist: pyspark (>=3.2.0,<4.0.0)
Description-Content-Type: text/markdown

docker run --rm -it -d -e PYTHONPATH="/ng-data-pipelines-sdk" -v $pwd:/ng-data-pipelines-sdk pyspark-local
