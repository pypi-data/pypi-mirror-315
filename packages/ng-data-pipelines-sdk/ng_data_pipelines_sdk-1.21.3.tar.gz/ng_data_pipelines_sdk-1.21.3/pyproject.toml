[tool.poetry]
name = "ng-data-pipelines-sdk"
version = "1.21.3"
description = "A library for interacting with data from Amazon S3 through PySpark. Read, write and transform data using a powerful and intuitive API with strong consistency and type checking, thanks to Pydantic. Compatible with Amazon MWAA running Airflow 2.7.2 and above."
authors = ["ng.cash"]
readme = "README.md"
exclude = [
    ".git/",
    ".gitignore",
    "tests/",
    "docs/",
    "*.yml",
    "__pycache__/",
    "*.pyc",
    "*.ipynb",
    "playground/",
    "poetry.lock",
    "dist/",
    "build/",
]

[tool.poetry.dependencies]
python = ">=3.9, <3.12"
pyspark = "^3.2.0"
pydantic = "^2.4.2"
boto3 = "^1.28.17"
pandas = "^2.1.1"
load-dotenv = "^0.1.0"
boto3-stubs = { version = "^1.35.52", extras = ["s3", "secretsmanager"] }
pydantic-settings = "^2.6.1"


[tool.poetry.group.dev.dependencies]
ipykernel = "^6.29.4"
ruff = "^0.3.7"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
