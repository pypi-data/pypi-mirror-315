Metadata-Version: 2.1
Name: powerchat-cli
Version: 0.1.6
Summary: CLI interface for local LLM assistant
Author-email: Quinten Roets <qdr2104@columbia.edu>
License: MIT
Project-URL: Source Code, https://github.com/quintenroets/chat
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: langchain-ollama<1,>=0.1.1
Requires-Dist: package-utils[context]<1,>=0.6.1
Requires-Dist: powercli<1,>=0.3.2
Provides-Extra: dev
Requires-Dist: package-dev-tools<1,>=0.5.11; extra == "dev"
Requires-Dist: package-dev-utils<1,>=0.1.6; extra == "dev"

# Chat
[![PyPI version](https://badge.fury.io/py/powerchat-cli.svg)](https://badge.fury.io/py/powerchat-cli)
![PyPI downloads](https://img.shields.io/pypi/dm/powerchat-cli)
![Python version](https://img.shields.io/badge/python-3.10+-brightgreen)
![Operating system](https://img.shields.io/badge/os-linux%20%7c%20macOS%20%7c%20windows-brightgreen)
![Coverage](https://img.shields.io/badge/coverage-100%25-brightgreen)

![example chat](https://github.com/quintenroets/chat/blob/main/assets/examples/example.png?raw=true)

## Usage

Run
```shell
chat
```
or
```shell
powerchat
```
## Installation
1. [Setup langchain-ollama](https://dev.to/emmakodes_/how-to-run-llama-31-locally-in-python-using-ollama-langchain-k8k)

2. Run `pip install powerchat-cli`
