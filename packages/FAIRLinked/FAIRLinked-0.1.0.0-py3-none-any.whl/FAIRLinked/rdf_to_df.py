import os
import json
import re
from rdflib import Graph, URIRef, Literal, Namespace
from rdflib.namespace import RDF, SKOS, DCTERMS, XSD
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

from FAIRLinked.utility import NAMESPACE_MAP

def parse_rdf_to_df(file_path: str,
                    variable_metadata_json_path: str,
                    arrow_output_path: str):
    """
    Parses an RDF Data Cube file (TTL or JSON-LD) generated by rdf_transformer back into a PyArrow Table and variable_metadata,
    then performs various enhancements:
    - Uses NAMESPACE_MAP from utility.py for creating Namespace objects.
    - Automatically determine RDF format from file extension.
    - Store variable_metadata as JSON.
    - Store PyArrow table (as Parquet) to the specified arrow_output_path.
    - Pretty print the first row and stats.
    - Sort variables by category and rows by ExperimentId.
    - If multiple units appear for a measure, store all unique units in a list under "Unit".

    Algorithm:
    1. Determine RDF format from file extension:
       - .ttl => 'turtle'
       - .jsonld or .json-ld => 'json-ld'
       Else default to 'turtle'.
    2. Parse RDF graph with rdflib.
    3. Identify qb:DataSet and qb:DataStructureDefinition (DSD).
    4. Extract dimensions and measures from the DSD:
       - Skip measureType dimension as a normal variable.
       - For each variable, extract altLabel, Category from property-level (Unit will be extracted from observations).
       - Initialize Unit as an empty list [].
    5. Map observation -> slice to retrieve dimension values from slice.
    6. For each qb:Observation:
       - Identify measure and measure value.
       - Retrieve dimension values from slice.
       - Retrieve unit from observation-level if present and append to that measure's Unit list if not already present.
    7. Construct a wide-format DataFrame.
    8. If 'ExperimentId' is present, sort rows by it.
    9. Sort columns by category, grouping variables of the same category together.
    10. Convert to PyArrow Table, save it as Parquet, and write variable_metadata as JSON.
    11. Print stats (rows, columns, categories) and preview the first row.

    Time Complexity:
    - O(T) to parse graph
    - O(D+M) to extract dimension/measure properties
    - O(S+O) to map observations and build data structure
    - O(R*M) for final DataFrame operations

    Missing Data/Edge Cases:
    - If no QB.DataSet or DSD found => ValueError
    - If no observations => empty DataFrame
    - Multiple units for a measure => all unique units stored in Unit list
    - Category at property-level, no category conflicts at observation-level

    Args:
        file_path (str): Path to RDF file (.ttl or .jsonld)
        variable_metadata_json_path (str): Path to store variable_metadata as JSON
        arrow_output_path (str): Path to store PyArrow table (Parquet file)

    Returns:
        pa.Table: The reconstructed table of observations.
                 Also writes variable_metadata to JSON and Parquet file.
    """

    # 1. Determine RDF format from file extension
    file_ext = os.path.splitext(file_path)[1].lower()
    if file_ext == '.ttl':
        rdf_format = 'turtle'
    elif file_ext in ('.jsonld', '.json-ld'):
        rdf_format = 'json-ld'
    else:
        print("Warning: Unknown extension. Defaulting to 'turtle' format.")
        rdf_format = 'turtle'

    # Create namespace objects from NAMESPACE_MAP
    ns_map = {}
    for prefix, uri in NAMESPACE_MAP.items():
        ns_map[prefix] = Namespace(uri if uri.endswith(('/', '#')) else uri + '#')

    QB = ns_map['qb']
    MEASURE_TYPE_URI = str(QB.measureType)
    category_prop_uri = URIRef(ns_map['mds']["category"])
    unit_measure_uri = URIRef(ns_map['sdmx-attribute']["unitMeasure"])

    # 2. Parse RDF graph
    g = Graph()
    g.parse(file_path, format=rdf_format)

    # 3. Identify qb:DataSet
    dataset_uri = None
    for s in g.subjects(RDF.type, QB.DataSet):
        dataset_uri = s
        break
    if dataset_uri is None:
        raise ValueError("No QB.DataSet found in the RDF file.")

    # Identify DSD
    dsd_uri = None
    for o in g.objects(dataset_uri, QB.structure):
        dsd_uri = o
        break
    if dsd_uri is None:
        raise ValueError("No QB.structure found for the DataSet.")

    dimensions = []
    measures = []
    variable_metadata = {}

    def uri_to_var_name(uri_str):
        uri_str = str(uri_str)
        part = uri_str.split('#')[-1]
        part = part.split('/')[-1]
        return part

    # 4. Extract dimensions and measures from DSD
    for component in g.objects(dsd_uri, QB.component):
        dim_prop = None
        measure_prop = None
        for p, o in g.predicate_objects(component):
            if p == QB.dimension:
                dim_prop = o
            elif p == QB.measure:
                measure_prop = o

        prop_uri = dim_prop if dim_prop else measure_prop
        if prop_uri is None:
            continue

        prop_types = list(g.objects(prop_uri, RDF.type))
        var_name = uri_to_var_name(prop_uri)
        prop_uri_str = str(prop_uri)

        # Skip measureType dimension
        if prop_uri_str == MEASURE_TYPE_URI:
            continue

        is_measure = False
        if QB.DimensionProperty in prop_types:
            dimensions.append(var_name)
        elif QB.MeasureProperty in prop_types:
            is_measure = True
            measures.append(var_name)

        # altLabel
        alt_label_obj = list(g.objects(prop_uri, SKOS.altLabel))
        alt_label = str(alt_label_obj[0]) if alt_label_obj else None

        # category
        category_obj = list(g.objects(prop_uri, category_prop_uri))
        category_val = None
        if category_obj:
            cat_str = str(category_obj[0])
            category_val = cat_str.split('#')[-1].split('/')[-1]

        # Unit initialized as empty list, can hold multiple units
        variable_metadata[var_name] = {
            "IsMeasure": "Yes" if is_measure else "No",
            "AltLabel": alt_label,
            "Category": category_val,
            "Unit": [],  # Now a list to accommodate multiple units
            "ExistingURI": prop_uri_str
        }

    # 5. Map observation -> slice
    observation_to_slice = {}
    for slice_uri in g.subjects(RDF.type, QB.Slice):
        for obs_uri in g.objects(slice_uri, QB.observation):
            observation_to_slice[obs_uri] = slice_uri

    # 6. Process observations
    all_observations = list(g.subjects(RDF.type, QB.Observation))
    dimension_grouped_data = {}

    for obs in all_observations:
        # Identify measure property
        measure_prop = None
        for o in g.objects(obs, QB.measureType):
            measure_prop = o
            break
        if measure_prop is None:
            continue

        measure_name = uri_to_var_name(measure_prop)
        if measure_name not in measures:
            continue

        # Get measure value
        measure_value = None
        for p, val in g.predicate_objects(obs):
            if p == measure_prop:
                measure_value = val.toPython() if isinstance(val, Literal) else str(val)
                break
        if measure_value is None:
            continue

        # Get dimension values
        slice_uri = observation_to_slice.get(obs)
        dim_values = {}
        if slice_uri:
            for dim_name in dimensions:
                dim_prop_uri = URIRef(variable_metadata[dim_name]["ExistingURI"])
                for obj in g.objects(slice_uri, dim_prop_uri):
                    dim_values[dim_name] = obj.toPython() if isinstance(obj, Literal) else str(obj)

        # Retrieve unit from observation, append if not already in the list
        for obj in g.objects(obs, unit_measure_uri):
            unit_val = str(obj)
            if unit_val not in variable_metadata[measure_name]["Unit"]:
                variable_metadata[measure_name]["Unit"].append(unit_val)

        # Add to data structure
        dim_key = tuple((dim, dim_values.get(dim)) for dim in dimensions)
        if dim_key not in dimension_grouped_data:
            dimension_grouped_data[dim_key] = dim_values.copy()
        dimension_grouped_data[dim_key][measure_name] = measure_value

    obs_data = list(dimension_grouped_data.values())
    df = pd.DataFrame(obs_data) if obs_data else pd.DataFrame()

    # Sort rows by ExperimentId if present
    if 'ExperimentId' in df.columns:
        df.sort_values(by='ExperimentId', inplace=True)

    # Sort columns by category then var_name
    var_categories = {}
    for var_name, meta in variable_metadata.items():
        cat = meta.get("Category")
        var_categories[var_name] = cat if cat else ""

    cols = list(df.columns)
    if 'ExperimentId' in cols:
        cols.remove('ExperimentId')
    cols.sort(key=lambda c: (var_categories.get(c, ""), c))
    if 'ExperimentId' in df.columns:
        df = df[['ExperimentId'] + cols]
    else:
        df = df[cols]

    # Convert to PyArrow Table
    table = pa.Table.from_pandas(df, preserve_index=False)

    # Store variable_metadata as JSON
    with open(variable_metadata_json_path, 'w', encoding='utf-8') as f:
        json.dump(variable_metadata, f, indent=2, ensure_ascii=False)

    # Store PyArrow table as Parquet
    pq.write_table(table, arrow_output_path)

    # Pretty print stats and first row
    num_rows = df.shape[0]
    num_cols = df.shape[1]
    distinct_categories = set(var_categories[var_name] for var_name in df.columns if var_categories[var_name])
    print("=== Conversion Stats ===")
    print(f"File: {file_path}")
    print(f"Rows (Experiments): {num_rows}")
    print(f"Columns (Variables): {num_cols}")
    print(f"Distinct Categories Found: {len(distinct_categories)} ({', '.join(distinct_categories)})")

    if num_rows > 0:
        print("\n=== First Row Preview ===")
        print(df.iloc[0].to_dict())
    else:
        print("\nNo data rows found.")

    return table, variable_metadata