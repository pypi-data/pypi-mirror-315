auto_trans_ckpt: false
auto_tune: false
autotune_per_step: 10
callbacks:
- type: MFLossMonitor
- async_save: false
  integrated_save: false
  prefix: llama2_7b
  save_checkpoint_steps: 100
  type: CheckpointMointor
- type: ObsMonitor
context:
  device_id: 0
  device_target: Ascend
  enable_graph_kernel: false
  graph_kernel_flags: --disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true
    --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true
  max_call_depth: 10000
  max_device_memory: 26GB
  mode: 0
  save_graphs: false
  save_graphs_path: ./graph
do_eval: false
eval_callbacks:
- type: ObsMonitor
eval_dataset: &id001
  data_loader:
    dataset_dir: ''
    shuffle: false
    type: MindDataset
  drop_remainder: false
  input_columns:
  - input_ids
  num_parallel_workers: 8
  numa_enable: false
  prefetch_size: 1
  python_multiprocessing: false
  repeat: 1
eval_dataset_task:
  dataset_config: *id001
  type: CausalLanguageModelDataset
filepath_prefix: ./autotune
init_start_profile: false
layer_decay: 0.65
layer_scale: false
load_checkpoint: ''
lr_scale_factor: 256
lr_schedule:
  learning_rate: 5.0e-05
  lr_end: 0
  total_steps: -1
  type: CosineWithWarmUpLR
  warmup_ratio: 0.03
metric:
  type: EmF1Metric
micro_batch_interleave_num: 1
model:
  arch:
    type: ParallelLlamaForCausalLM
  model_config:
    batch_size: 1
    block_size: 16
    bos_token_id: 1
    checkpoint_name_or_path: llama2_7b
    compute_dtype: float16
    do_sample: false
    eos_token_id: 2
    extend_method: None
    hidden_size: 4096
    ignore_token_id: -100
    is_dynamic: true
    layernorm_compute_type: float16
    max_decode_length: 1024
    multiple_of: 256
    num_blocks: 1024
    num_heads: 32
    num_layers: 32
    offset: 0
    pad_token_id: 0
    param_init_type: float16
    qkv_concat: True
    quantization_config:
      activation_dtype: int8
      algorithm_args: {}
      kvcache_dtype: int8
      modules_to_not_convert:
      - lm_head
      outliers_suppression: smooth
      quant_method: ptq
      weight_dtype: int8
    repetition_penalty: 1
    rms_norm_eps: 1.0e-05
    rotary_dtype: float16
    scaling_factor: 1.0
    seq_length: 4096
    softmax_compute_type: float16
    top_k: 3
    top_p: 1
    type: LlamaConfig
    use_flash_attention: true
    use_past: true
    vocab_size: 32000
only_save_strategy: false
optimizer:
  beta1: 0.9
  beta2: 0.95
  eps: 1.0e-08
  learning_rate: 5.0e-05
  type: FP32StateAdamWeightDecay
output_dir: ./output/
parallel:
  enable_alltoall: false
  enable_parallel_optimizer: true
  full_batch: false
  gradients_mean: false
  parallel_mode: STAND_ALONE
  parallel_optimizer_config:
    gradient_accumulation_shard: false
    parallel_optimizer_threshold: 64
  search_mode: sharding_propagation
  strategy_ckpt_save_file: ./ckpt_strategy.ckpt
parallel_config:
  data_parallel: 1
  gradient_aggregation_group: 4
  micro_batch_num: 1
  model_parallel: 4
  pipeline_stage: 1
  use_seq_parallel: false
  vocab_emb_dp: false
processor:
  return_tensors: ms
  tokenizer:
    bos_token: <s>
    eos_token: </s>
    pad_token: <unk>
    type: LlamaTokenizer
    unk_token: <unk>
    vocab_file: /load/0923/ckpt_convert/model/llama2_7b/tokenizer.model
  type: LlamaProcessor
profile: false
profile_communication: false
profile_memory: true
profile_start_step: 1
profile_stop_step: 10
recompute_config:
  mp_comm_recompute: true
  parallel_optimizer_comm_recompute: false
  recompute: false
  recompute_slice_activation: true
  select_recompute: false
remote_save_url: Please input obs url on AICC platform.
resume_training: false
run_mode: predict
runner_config:
  batch_size: 1
  epochs: 2
  gradient_accumulation_steps: 8
  sink_mode: true
  sink_size: 2
runner_wrapper:
  scale_sense:
    loss_scale_value: 65536
    scale_factor: 2
    scale_window: 1000
    type: DynamicLossScaleUpdateCell
  type: MFTrainOneStepCell
  use_clip_grad: true
seed: 0
src_strategy_path_or_dir: ''
train_dataset: &id002
  batch_size: 6
  data_loader:
    dataset_dir: ''
    shuffle: true
    type: MindDataset
  drop_remainder: true
  input_columns:
  - input_ids
  num_parallel_workers: 8
  numa_enable: false
  prefetch_size: 1
  python_multiprocessing: false
  repeat: 1
train_dataset_task:
  dataset_config: *id002
  type: CausalLanguageModelDataset
trainer:
  model_name: llama2_7b
  type: CausalLanguageModelingTrainer
use_parallel: true
