{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import semchunk\n",
    "from transformers import AutoTokenizer # Neither `transformers` nor `tiktoken` are required,\n",
    "import tiktoken                        # they are here for demonstration purposes.\n",
    "\n",
    "chunk_size = 2 # A low chunk size is used here for demonstration purposes.\n",
    "text = 'The quick brown fox jumps over the lazy dog.'\n",
    "\n",
    "# As you can see below, `semchunk.chunkerify` will accept the names of all OpenAI models, OpenAI\n",
    "# `tiktoken` encodings and Hugging Face models (in that order of precedence), along with custom\n",
    "# tokenizers that have an `encode()` method (such as `tiktoken`, `transformers` and `tokenizers`\n",
    "# tokenizers) and finally any function that can take a text and return the number of tokens in it.\n",
    "chunker = semchunk.chunkerify('umarbutler/emubert', chunk_size) or \\\n",
    "          semchunk.chunkerify('gpt-4', chunk_size) or \\\n",
    "          semchunk.chunkerify('cl100k_base', chunk_size) or \\\n",
    "          semchunk.chunkerify(AutoTokenizer.from_pretrained('umarbutler/emubert'), chunk_size) or \\\n",
    "          semchunk.chunkerify(tiktoken.encoding_for_model('gpt-4'), chunk_size) or \\\n",
    "          semchunk.chunkerify(lambda text: len(text.split()), chunk_size)\n",
    "\n",
    "# The resulting `chunker` can take and chunk a single text or a list of texts, returning a list of\n",
    "# chunks or a list of lists of chunks, respectively.\n",
    "assert chunker(text) == ['The quick', 'brown', 'fox', 'jumps', 'over the', 'lazy', 'dog.']\n",
    "assert chunker([text], progress = True) == [['The quick', 'brown', 'fox', 'jumps', 'over the', 'lazy', 'dog.']]\n",
    "\n",
    "# If you have a large number of texts to chunk and speed is a concern, you can also enable\n",
    "# multiprocessing by setting `processes` to a number greater than 1.\n",
    "assert chunker([text], processes = 2) == [['The quick', 'brown', 'fox', 'jumps', 'over the', 'lazy', 'dog.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tiktoken.core.Encoding"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "sys.path.append('D:/workspace/semchunk/')\n",
    "from importlib import reload\n",
    "\n",
    "import tiktoken\n",
    "import semchunk as older_semchunk\n",
    "import src.semchunk.semchunk as newer_semchunk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "reload(newer_semchunk)\n",
    "\n",
    "newer_semchunk.chunk = newer_semchunk.chunk.__wrapped__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.encoding_for_model('gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chunker = newer_semchunk.chunkerify('umarbutler/emubert', 512, memoize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.encode('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Protocol, runtime_checkable\n",
    "\n",
    "@runtime_checkable\n",
    "class Tokeniser(Protocol):\n",
    "    model_max_length: str\n",
    "\n",
    "@runtime_checkable\n",
    "class Tokeniser2(Protocol):\n",
    "    a:1\n",
    "\n",
    "isinstance(tokeniser, Tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\\\n",
    "Investing in access to justice and improving community safety\n",
    "You are hereHome  Media centre  Investing in access to justice and improving community safety\n",
    "The Hon Mark Dreyfus KC MP\n",
    "14 May 2024  \n",
    "Media Release\n",
    "The Albanese Labor Government is delivering significant investments to strengthen community safety, restore accountability and integrity in government and improve access to justice for all Australians.  \n",
    "\n",
    "Improving Australians’ access to justice\n",
    "The Albanese Government is providing an urgent injection of $44.1 million in 2024-25 to provide an immediate funding boost to the legal assistance sector to ensure that more Australians have access to justice and equality before the law.  \n",
    "\n",
    "The current National Legal Assistance Partnership (NLAP) expires in June 2025. We have heard the sector's call for a funding boost before the NLAP expires. Together with state and territory governments we are currently considering a review of the NLAP.\n",
    "\n",
    "Keeping Australians safe\n",
    "The Albanese Government is delivering significant investments to strengthen community safety, including improving information sharing to better equip law enforcement to tackle crime such as gender-based violence.\n",
    "\n",
    "The Albanese Government has committed $109.9 million to further enhance the National Criminal Intelligence System (NCIS) and improve information-sharing nationally to allow for better law enforcement responses to criminal activity.\n",
    "\n",
    "The NCIS provides law enforcement officers across Australia with near real time access to cross-border policing information and criminal intelligence.\n",
    "\n",
    "The NCIS provides opportunities to enhance the safety of women and children by providing police officers with access to key information from multiple agencies and systems on a single national database, allowing them to see relevant risk information such as domestic and family violence orders or outstanding warrants.  \n",
    "\n",
    "This secure system enables front-line officers to access information they need, when they need it, to address and prevent criminal activity.\n",
    "\n",
    "The Albanese Government has also invested an additional $11 million in the 2024-25 Budget for a mobile app and secure website that will enable all Australians to easily and swiftly protect their identity credentials from cyber criminals, building on the success of the Credential Protection Register.\n",
    "\n",
    "We have also announced $161.3 million over four years to establish the National Firearms Register, and to support extensive reform of Commonwealth, state and territory firearms management systems.\n",
    "\n",
    "The National Firearms Register is the most significant improvement in Australia's firearms management systems in almost 30 years and will keep our community and police safer.\n",
    "\n",
    "As announced last month, we are committing an additional $14.2 million over two financial years from 2024–25 to improve community safety in Alice Springs and surrounds. This funding will deliver additional policing and other community safety support measures as part of our commitment to Strengthening Community Safety in Central Australia.\n",
    "\n",
    "Tackling transnational, serious and organised crime\n",
    "After nearly a decade of Coalition mismanagement leaving Australians vulnerable to drug trafficking, terrorism financing and child exploitation, the Albanese Government is committed to taking action to ensure that Australians are safe from serious crime.\n",
    "\n",
    "We are investing $166.4 million to implement proposed reforms to Australia’s anti-money laundering and counter-terrorism financing regime, including to support businesses by delivering comprehensive education and guidance.  \n",
    "\n",
    "As part of broader efforts to tackle transnational, serious and organised crime, we are also boosting capabilities with a further $48.7 million in funding for the Australian Criminal Intelligence Commission to enhance operational activities and core capabilities.\n",
    "\n",
    "Restoring integrity and accountability to government decision-making\n",
    "The Albanese Government is investing $206.5 million over four years to deliver a user-focused, accessible, efficient and independent Administrative Review Tribunal.  \n",
    "\n",
    "Each year, thousands of Australians will rely on the new Tribunal to independently review government decisions that have major, life-altering impacts – decisions such as whether an older Australian receives an age pension, whether a veteran is compensated for a service injury, or whether a participant of the NDIS receives funding for essential support.\n",
    "\n",
    "The Government will provide the new Tribunal with flexible, demand-driven funding based on case lodgements, which will allow the Tribunal to proactively manage its resources to enable timely decision-making across all parts of the Tribunal’s jurisdiction and resolve the cases it receives.\n",
    "\n",
    "The Albanese Labor Government’s Administrative Review Tribunal will deliver real and lasting benefits for thousands of Australians and improve government decision making.  \n",
    "\n",
    "Restoring integrity to refugee protection\n",
    "The Albanese Government will provide $115.6 million over four years to restore integrity to Australia’s refugee protection system, providing a fair go to genuine asylum seekers and helping to break the business model of people who seek to exploit the system.  \n",
    "\n",
    "The previous government oversaw record numbers of onshore Protection visa applications and a system that was not equipped to deal with them.\n",
    "\n",
    "The Nixon Report found that delays in processing – and reviewing – Protection visa applications were ‘motivating bad actors to take advantage by lodging increasing numbers of non-genuine applications for protection’, which is why the Government will invest in:\n",
    "\n",
    "The establishment of two “migration hubs” in Western Sydney and Melbourne – innovative new court facilities that will be dedicated to hearing, and expeditiously resolving, migration and protection matters; and\n",
    "An additional eight judges – four in the Federal Circuit and Family Court and four in the Federal Court – to resolve the significant backlogs of migration and protection applications left by the previous government.\n",
    "This investment builds on the $163.5 million package of reforms the Government announced in October 2023.\n",
    "\n",
    "Investing in better legal outcomes for Aboriginal and Torres Strait Islander people\n",
    "The Albanese Government is investing $20.8 million to improve outcomes for First Nations people at all stages of the native title process.\n",
    "\n",
    "$20.2 million over four years to the Federal Court of Australia and National Native Title Tribunal to preserve culturally and historically significant native title records and accelerate the resolution of unresolved native title claims through the expansion of its case management and mediation model, which works with First Nations people to achieve the outcomes they want for their communities; and\n",
    "$0.5 million over two years to assist the Australian Law Reform Commission to review the future acts regime within the Native Title Act as recommended in the final inquiry of the Joint Standing Committee on Northern Australia into the destruction of 46,000-year-old caves in Western Australia.\n",
    "We are also extending the First Nations Family Dispute Resolution pilot program for a further two years, committing $11.7 million to ensure culturally specific and appropriate dispute resolution services can be embedded in the community.\n",
    "\n",
    "Combatting Modern Slavery\n",
    "We are committed to tackling modern slavery overseas and at home. We are investing $2.5 million to deliver Labor’s election commitment to undertake an audit of Federal Government procurement procedures and supply chains.  \n",
    "\n",
    "We believe the Commonwealth should lead by example. This audit will act as a blueprint for future action from governments to review their supply chains and ensure they are not importing goods that are the product of forced labour.\n",
    "\n",
    "Mapping (almost) every law, regulation and case in Australia\n",
    "Mar 21, 2024\n",
    "Data 📊, Law ⚖️\n",
    "What if you could take every law, regulation and case in Australia and project them onto a two-dimensional map such that their distance from one another was proportional to their similarity in meaning? What would that look like?\n",
    "\n",
    "Perhaps something like this.\n",
    "\n",
    "This is the first ever map of Australian law. Each point represents a unique law, regulation or case in the Open Australian Legal Corpus, the world’s largest open source database of Australian law (you can learn about how I built that corpus here).\n",
    "\n",
    "The closer any two documents are on the map, the more similar they are in meaning.\n",
    "\n",
    "If you’re on a computer and hover over a document, you’ll see its title, type, jurisdiction and category. You can open a document by clicking on it.\n",
    "\n",
    "Documents are coloured by category. The legend on the right shows what colour each category correspond to. Click on a category and you’ll exclude it from the map. Double click and you’ll only see documents from that category.\n",
    "\n",
    "Over the course of this article, I’ll cover exactly what the map can teach us about Australian law as well as give you a behind-the-scenes look at how I built it, providing code examples along the way. Those more interested in the technology powering the map can skip straight to that section here.\n",
    "\n",
    "What can we learn from it?\n",
    "While it might not look like much at first, the map gives us a rare look into some of the many hidden ways Australian laws, regulations and cases are both connected to and disconnected from one another.\n",
    "\n",
    "The invisible barrier between cases and legislation\n",
    "It is readily observable, for example, that there is a sort of invisible barrier separating cases on the one hand from legislation on the other. This barrier corresponds roughly with the map’s north and south poles.\n",
    "\n",
    "\n",
    "An annotated version of the map where cases and legislation are enclosed in two shapes corresponding with the map’s north and south poles, respectively.\n",
    "The presence of this barrier tells us that documents of the same type will tend to share more in common with each other than they will with documents of the same subject matter.\n",
    "\n",
    "Although they may often focus on the same topics, cases and legislation are, after all, written in different styles, towards different ends.\n",
    "\n",
    "The absence of borders between documents of different jurisdictions\n",
    "Interestingly, however, we find no such borders between documents of different jurisdictions; although, it is worth noting, due to copyright restrictions, the Open Australian Legal Corpus only contains decisions from the Commonwealth and New South Wales and is missing legislation from Victoria, the Northern Territory and the Australian Capital Territory.\n",
    "\n",
    "\n",
    "An alternate view of the map where documents are coloured by jurisdiction, illustrating the lack of boundaries between documents of different jurisdictions.\n",
    "The absence of borders between cases and legislation of different jurisdictions indicates that Australian state and federal law is relatively homogenous. There are no differences between the style, principles of interpretation or general jurisprudence of state and federal law that appear to be reflected in the map. Of what borders do exist between state and federal law, they correspond better with differences in subject matter than they do with the jurisprudence of their jurisdictions.\n",
    "\n",
    "This conforms with fact that state and federal courts and legislatures operate within a single legal framework, under which they have jurisdiction over matters prescribed by the Constitution in their territory, with a single court, the High Court of Australia, arbitrating on disputes between governments over the precise limits of those constitutional rights and powers.\n",
    "\n",
    "The judicial and legislative mainlands and islands\n",
    "Turning back to the barrier between cases and legislation, we also observe that, within the map’s north and south poles, each pole has a ‘mainland’ of sorts that most documents belong to, and then there are a range of ‘islands’ that orbit those mainlands, typically consisting of documents of the same subject matter.\n",
    "\n",
    "\n",
    "An annotated version of the map where ‘islands’ of documents of the same subject matter are enclosed in shapes that orbit ‘mainlands’ of cases and legislation.\n",
    "The fact that there are judicial and legislative mainlands suggests that most cases and legislation draw from and feed into a single, interconnected pool of knowledge.\n",
    "\n",
    "This is not particularly surprising. What is surprising is that there are large islands of legislation and judgments that are entirely cut off from their respective mainlands.\n",
    "\n",
    "Tariff concession orders, for example, form their very own unique archipelago, perhaps because each order is centred around regulating a distinct, often quite technical class of importable goods, from magazine holders to forklifts.\n",
    "\n",
    "There is also quite a sizeable island of airworthiness directives primarily focused on regulating aircraft components, another highly technical domain.\n",
    "\n",
    "Somewhat unexpectedly, the largest island by surface area consists almost entirely of migration cases. Furthermore, of all 19 possible branches of law, migration and family law are the only two to be found more often outside a mainland than inside one.\n",
    "\n",
    "Migration and family law are, in effect, the most isolated areas of Australian law on the map.\n",
    "\n",
    "Funnily enough, while researching why that might be, I stumbled upon this rather pertinent quote from Lord Sumption:\n",
    "\n",
    "Courts exercising family jurisdiction do not occupy a desert island in which general legal concepts are suspended or mean something different. If a right of property exists, it exists in every division of the High Court and in every jurisdiction of the county courts. If it does not exist, it does not exist anywhere.\n",
    "\n",
    "Prest v Petrodel Resources Ltd [2013] UKSC 34, [37] (emphasis added)\n",
    "\n",
    "I also discovered that Munby LJ, later President of the UK Family Division, had likewise once quipped:\n",
    "\n",
    "The Family Division is part of the High Court. It is not some legal Alsatia where the common law and equity do not apply. The rules of agency apply there as much as elsewhere. But in applying those rules one must have regard to the context …\n",
    "\n",
    "Richardson v Richardson [2011] EWCA Civ 79, [53]\n",
    "\n",
    "It would seem that there was already a perception that family law is somewhat isolated from the rest of the law, which the map appears to support.\n",
    "\n",
    "As for migration law, although I was unable to locate equally apropos quotes, from my own review of a selection of cases on the map, they appear relatively self-contained in that they tend to reference legislation and cases particular to migration law. It also makes sense that migration law would be a little distant from other areas of law given its unique subject matter.\n",
    "\n",
    "While not as insular as family and migration law, it is also worth addressing the relatively large hexagram-shaped island of criminal law, which features a tail of transport and administrative law cases coming out of it.\n",
    "\n",
    "That island appears to consist mostly of substantive criminal law cases (along with certain punitive transport and administrative law cases focused on the suspension of various types of licences), whereas the criminal law cases connected to the judicial mainland tend to concern criminal procedure.\n",
    "\n",
    "\n",
    "An annotated version of criminal law cases with the island of substantive criminal law cases and the cluster of procedural criminal law cases connected to the judicial mainland enclosed in their own shapes. Only light blue data points are criminal cases.\n",
    "This supports the broad division of substantive law into criminal law and civil law while also conforming well with the fact that criminal procedure law and civil procedure law share a number of common principles of natural justice.\n",
    "\n",
    "The most and least legislative areas of judicial law\n",
    "Fascinatingly, migration, family and substantive criminal law also all tend to cluster closely together latitudinally, hinting at potential hidden connection. They are all known to overlap in certain ways and they all share a special focus on regulating the lives of individuals, and not merely the property rights of legal persons.\n",
    "\n",
    "Migration, family and substantive criminal law cases also all happen to be the most distant types of cases from legislation on the map. Of course, this does not mean that they never cite legislation, but it may be that they rely on precedent more often than other areas of case law. It might also be the result of the inherent difficulty in attempting to represent highly complex and multidimensional relationships in a simple two-dimensional map.\n",
    "\n",
    "Conversely, the class of cases closest to legislation is development cases, which makes sense since they can often deal quite intimately with local planning laws and regulations.\n",
    "\n",
    "The case law continuum\n",
    "If we start at the bottom of the cases mainland and make our way up, we can also see that Australian case law is a continuum of sorts.\n",
    "\n",
    "\n",
    "An annotated version of the case law mainland where select branches of law are pointed out, illustrating the continuum of case law.\n",
    "Development cases connect with environmental cases, which then link with land cases.\n",
    "\n",
    "Land cases border contract cases which in turn have procedural cases to their north, intellectual property cases to their west and commercial cases to their east.\n",
    "\n",
    "Moving further north of procedural law brings you to criminal law and defamation.\n",
    "\n",
    "Heading west from intellectual property law takes you through administrative law, health and social services law, employment law, negligence and finally transport law.\n",
    "\n",
    "Going east of commercial law, you’ll find equity and a subset of family law.\n",
    "\n",
    "\n",
    "An animation of branches of law appearing on the map sequentially, illustrating the continuum of case law.\n",
    "This continuum corresponds well with our pre-existing understandings of the relationships between the various branches of the law.\n",
    "\n",
    "It makes sense, for example, that development, environment and land law would all be intertwined given their similar subject matter. Likewise, it is not at all unexpected that negligence would cluster closely with transport and employment law when a great many negligence cases centre around motor and workplace accident claims.\n",
    "\n",
    "The map, in effect, crystallises our own mental models of the law.\n",
    "\n",
    "It also shows us that the borders between various areas of the law can often be quite porous. We notice, for instance, that there is a streak of land law judgments that overlaps with commercial and procedural law cases and is disconnected from most other land law cases. Interestingly, cases in this streak tend to focus on mortgage disputes often involving defaults which would explain why they overlap with commercial and procedural law cases.\n",
    "\n",
    "We can also see that there are some transport law judgments that are connected to the cases mainland and then there are others that are connected to the island of substantive criminal law cases. Transport judgments connected to that island often centre around the suspension of transport licences, whereas judgments connected to the cases mainland tend to focus on transport accidents. Although disconnected from one another, however, both clusters of transport cases are still relatively close to each other, reflecting their shared subject matter.\n",
    "\n",
    "Final thoughts\n",
    "By now, we’ve covered how the map reflects already known distinctions between cases and legislation, while also revealing potential new divisions and hidden connections between various areas of the law. We’ve also seen how Australian case law can be more of a continuum than a rigidly defined structure and how the borders between branches of case law can often be quite porous.\n",
    "\n",
    "Other specific insights we’ve been able to find are that:\n",
    "\n",
    "Migration, family and substantive criminal law are the most isolated branches of case law on the map;\n",
    "Migration, family and substantive criminal law are the most distant branches of case law from legislation on the map;\n",
    "Development law is the closest branch of case law to legislation on the map; and\n",
    "The map does not reveal any noticeable distinctions between Australian state and federal law, whether it be in style, principles of interpretation or general jurisprudence.\n",
    "These are but a selection of the most readily observable insights to be gained from attempting to map Australian law. There are no doubt countless others waiting to be uncovered. Producing a three-dimensional map of Australian laws, cases and regulations could, for example, reveal new hidden relationships that are almost impossible to represent in two dimensions. Adding cases and legislation from other states and territories might also give us a sharper, higher resolution image of the map, deepening our understanding of the geography of Australian law. One could even imagine adding legal documents from other common law countries such as the UK, Canada and New Zealand to, in a sense, photograph the historic and continued interactions between our legal systems.\n",
    "\n",
    "Nevertheless, for a first attempt, the map already has a lot to teach us. Perhaps you’ve even identified patterns in the map that I could not.\n",
    "\n",
    "The greatest thing about this exercise is that it can be applied to virtually any domain, not just Australian law. Semantic mapping is particularly useful for very quickly developing an understanding of the underlying composition and structure of a dataset without having to manually scour through hundreds of examples to build your own much noisier and less persistent mental model of that data.\n",
    "\n",
    "Since finishing the map, I’ve already been able to reuse this technique to study countless other seemingly unstructured large datasets, and you can too. It doesn’t take an expert in clustering and mapping to pull it off. Far from it. Prior to starting this project, I didn’t know the first thing about semantic mapping and now I’m about to teach you how to do it yourself.\n",
    "\n",
    "So how’d you do it?\n",
    "At a high level, the process for mapping any arbitrary set of data points, whether they be PDFs, YouTube videos, TikToks or anything else, can be broken down into six stages, illustrated below.\n",
    "\n",
    "\n",
    "An illustration of the process of semantically mapping data.\n",
    "In brief, we try to represent the meaning of data in the form of sets of numbers (vectorisation), after which we group those sets into clusters based on their similarity (clustering) and subsequently label those clusters based on whatever unique patterns we can find in them (labelling). Finally, we project the numerical representations of the data into two-dimensional coordinates (dimensionality reduction) which we then plot on a map (visualisation).\n",
    "\n",
    "Through this next section, we’ll take a deeper look at exactly how every step of the semantic mapping process works in practice. Before that though, I’d like to express my gratitude to the creators of BERTopic, a topic modelling technique which this process was loosely based on, as well as Dr Mike DeLong whose topic map of the Open Australian Legal Corpus served as the inspiration for this entire project.\n",
    "\n",
    "Vectorisation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.7 ms ± 172 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "33.6 ms ± 286 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "def tok_counter(text: str) -> int:\n",
    "    return len(tokeniser.encode(text, add_special_tokens = False))\n",
    "\n",
    "%timeit chunker(text)\n",
    "%timeit newer_semchunk.chunk(text, 512, tok_counter, memoize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\umarb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "gutenberg = nltk.corpus.gutenberg\n",
    "encoding = tiktoken.encoding_for_model('gpt-4')\n",
    "encoding = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def _token_counter(text: str) -> int:\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== austen-emma.txt ====\n",
      "Old semchunk tokens: 221,851\n",
      "Old semchunk chars: 886,322\n",
      "\n",
      "New semchunk tokens: 221,851\n",
      "New semchunk chars: 886,322\n",
      "\n",
      "Original tokens: 223,077\n",
      "Original chars: 887,071\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== austen-persuasion.txt ====\n",
      "Old semchunk tokens: 112,528\n",
      "Old semchunk chars: 465,914\n",
      "\n",
      "New semchunk tokens: 112,528\n",
      "New semchunk chars: 465,914\n",
      "\n",
      "Original tokens: 113,153\n",
      "Original chars: 466,292\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== austen-sense.txt ====\n",
      "Old semchunk tokens: 167,352\n",
      "Old semchunk chars: 672,405\n",
      "\n",
      "New semchunk tokens: 167,352\n",
      "New semchunk chars: 672,405\n",
      "\n",
      "Original tokens: 168,281\n",
      "Original chars: 673,022\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== bible-kjv.txt ====\n",
      "Old semchunk tokens: 1,195,105\n",
      "Old semchunk chars: 4,329,383\n",
      "\n",
      "New semchunk tokens: 1,195,105\n",
      "New semchunk chars: 4,329,383\n",
      "\n",
      "Original tokens: 1,200,627\n",
      "Original chars: 4,332,554\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== blake-poems.txt ====\n",
      "Old semchunk tokens: 11,233\n",
      "Old semchunk chars: 38,133\n",
      "\n",
      "New semchunk tokens: 11,233\n",
      "New semchunk chars: 38,133\n",
      "\n",
      "Original tokens: 11,250\n",
      "Original chars: 38,153\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== bryant-stories.txt ====\n",
      "Old semchunk tokens: 70,408\n",
      "Old semchunk chars: 248,360\n",
      "\n",
      "New semchunk tokens: 70,408\n",
      "New semchunk chars: 248,360\n",
      "\n",
      "Original tokens: 71,696\n",
      "Original chars: 249,439\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== burgess-busterbrown.txt ====\n",
      "Old semchunk tokens: 22,723\n",
      "Old semchunk chars: 84,242\n",
      "\n",
      "New semchunk tokens: 22,723\n",
      "New semchunk chars: 84,242\n",
      "\n",
      "Original tokens: 23,232\n",
      "Original chars: 84,663\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== carroll-alice.txt ====\n",
      "Old semchunk tokens: 39,455\n",
      "Old semchunk chars: 144,270\n",
      "\n",
      "New semchunk tokens: 39,455\n",
      "New semchunk chars: 144,270\n",
      "\n",
      "Original tokens: 39,624\n",
      "Original chars: 144,395\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== chesterton-ball.txt ====\n",
      "Old semchunk tokens: 116,482\n",
      "Old semchunk chars: 457,123\n",
      "\n",
      "New semchunk tokens: 116,482\n",
      "New semchunk chars: 457,123\n",
      "\n",
      "Original tokens: 117,092\n",
      "Original chars: 457,450\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== chesterton-brown.txt ====\n",
      "Old semchunk tokens: 104,326\n",
      "Old semchunk chars: 406,320\n",
      "\n",
      "New semchunk tokens: 104,326\n",
      "New semchunk chars: 406,320\n",
      "\n",
      "Original tokens: 104,401\n",
      "Original chars: 406,629\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== chesterton-thursday.txt ====\n",
      "Old semchunk tokens: 80,775\n",
      "Old semchunk chars: 320,307\n",
      "\n",
      "New semchunk tokens: 80,775\n",
      "New semchunk chars: 320,307\n",
      "\n",
      "Original tokens: 81,156\n",
      "Original chars: 320,525\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== edgeworth-parents.txt ====\n",
      "Old semchunk tokens: 265,156\n",
      "Old semchunk chars: 933,158\n",
      "\n",
      "New semchunk tokens: 265,156\n",
      "New semchunk chars: 933,158\n",
      "\n",
      "Original tokens: 267,794\n",
      "Original chars: 935,158\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== melville-moby_dick.txt ====\n",
      "Old semchunk tokens: 340,660\n",
      "Old semchunk chars: 1,239,056\n",
      "\n",
      "New semchunk tokens: 340,660\n",
      "New semchunk chars: 1,239,056\n",
      "\n",
      "Original tokens: 345,552\n",
      "Original chars: 1,242,990\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== milton-paradise.txt ====\n",
      "Old semchunk tokens: 131,148\n",
      "Old semchunk chars: 468,220\n",
      "\n",
      "New semchunk tokens: 131,148\n",
      "New semchunk chars: 468,220\n",
      "\n",
      "Original tokens: 131,431\n",
      "Original chars: 468,220\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== shakespeare-caesar.txt ====\n",
      "Old semchunk tokens: 35,891\n",
      "Old semchunk chars: 112,225\n",
      "\n",
      "New semchunk tokens: 35,891\n",
      "New semchunk chars: 112,225\n",
      "\n",
      "Original tokens: 35,909\n",
      "Original chars: 112,310\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== shakespeare-hamlet.txt ====\n",
      "Old semchunk tokens: 51,644\n",
      "Old semchunk chars: 162,751\n",
      "\n",
      "New semchunk tokens: 51,644\n",
      "New semchunk chars: 162,751\n",
      "\n",
      "Original tokens: 51,668\n",
      "Original chars: 162,881\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== shakespeare-macbeth.txt ====\n",
      "Old semchunk tokens: 32,544\n",
      "Old semchunk chars: 100,253\n",
      "\n",
      "New semchunk tokens: 32,544\n",
      "New semchunk chars: 100,253\n",
      "\n",
      "Original tokens: 32,597\n",
      "Original chars: 100,351\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n",
      "==== whitman-leaves.txt ====\n",
      "Old semchunk tokens: 194,073\n",
      "Old semchunk chars: 710,271\n",
      "\n",
      "New semchunk tokens: 194,073\n",
      "New semchunk chars: 710,271\n",
      "\n",
      "Original tokens: 195,157\n",
      "Original chars: 711,215\n",
      "\n",
      "Old semchunk is the same as new semchunk: Yes\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    text = gutenberg.raw(fileid)\n",
    "    \n",
    "    chunk_size = 512\n",
    "    joiner = ' '\n",
    "    # text = joiner.join(['It is I who is him who is I']*100)\n",
    "    # chunk_size = 1\n",
    "\n",
    "    old = older_semchunk.semchunk.chunk(text, chunk_size, token_counter=_token_counter)\n",
    "    newer = newer_semchunk.chunk(text, chunk_size, token_counter=_token_counter)\n",
    "    # newest = newest_semchunk.chunk(text, chunk_size, token_counter=_token_counter)\n",
    "\n",
    "    ol = len(encoding.encode(joiner.join(old))), len(joiner.join(old))\n",
    "    nl = len(encoding.encode(joiner.join(newer))), len(joiner.join(newer))\n",
    "    # nsl = len(encoding.encode(joiner.join(newest))), len(joiner.join(newest))\n",
    "    ogl = len(encoding.encode(text)), len(text)\n",
    "\n",
    "    print(f'==== {fileid} ====')\n",
    "    print(f\"Old semchunk tokens: {ol[0]:,}\")\n",
    "    print(f\"Old semchunk chars: {ol[1]:,}\")\n",
    "    print()\n",
    "    print(f\"New semchunk tokens: {nl[0]:,}\")\n",
    "    print(f\"New semchunk chars: {nl[1]:,}\")\n",
    "    print()\n",
    "    # print(f\"Newest semchunk tokens: {nsl[0]:,}\")\n",
    "    # print(f\"Newest semchunk chars: {nsl[1]:,}\")\n",
    "    # print()\n",
    "    print(f\"Original tokens: {ogl[0]:,}\")\n",
    "    print(f\"Original chars: {ogl[1]:,}\")\n",
    "    print()\n",
    "    print(\"Old semchunk is the same as new semchunk:\", 'Yes' if old == newer else 'No')\n",
    "    # print(\"Old semchunk is the same as newest semchunk:\", 'Yes' if old == newest else 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old == newer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [len(encoding.encode(c)) for c in newest]\n",
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old == newer == newest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000, 10000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old), len(newer), len(newest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Hi Hi' in newer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newest == newer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================================',\n",
       " '=================================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================================',\n",
       " '=================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=================',\n",
       " '=====']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================================',\n",
       " '================================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================',\n",
       " '================']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newer_semchunk.chunk('='*10000, 1, token_counter=_token_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding.encode('='*10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(encoding.encode(x)) for x in newer_semchunk.chunk('='*10000, 1, token_counter=_token_counter)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(encoding.encode(x)) for x in older_semchunk.chunk('='*10000, 1, token_counter=_token_counter)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(text: str) -> int:\n",
    "    \"\"\"Count the number of tokens in a text\"\"\"\n",
    "\n",
    "    if text == '\"' * 5000:\n",
    "        return 1\n",
    "\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split(splits: list[str], max_size: int, splitter: str, counter: callable) -> tuple[int, str]:\n",
    "    \"\"\"Binary search for the optimal split point where the accumulated_token_count < max_size.\"\"\"\n",
    "    \n",
    "    low, high = 0, len(splits) + 1\n",
    "    \n",
    "    while low < high:\n",
    "        # As the main performance hit comes from running the token_counter on long texts\n",
    "        # we can bias the binary search to favour guessing towards shorter sequences.\n",
    "        # This is done below by using > 2 as the divisor\n",
    "        mid = low + (high - low) // 8\n",
    "        if counter(splitter.join(splits[:mid])) > max_size:\n",
    "            high = mid\n",
    "        else:\n",
    "            low = mid + 1\n",
    "\n",
    "    return low - 1, splitter.join(splits[:low - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And I said to her, hello', 'govna, hello maam, hello hello.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semchunk.chunk(\"\"\"And I said to her, hello govna, hello maam, hello hello.\"\"\", chunk_size=6, token_counter=lambda text: len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And I said to her, hello', 'govna, hello maam, hello hello.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "older_semchunk.chunk(\"\"\"And I said to her, hello govna, hello maam, hello hello.\"\"\", chunk_size=6, token_counter=lambda text: len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = semchunk._split_text(\"\"\"And I said to her, hello govna, hello maam, hello hello.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 'And I')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_split(s[-1], 6, ' ', counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(text: str, max_size: int, counter: callable) -> int:\n",
    "    \"\"\"Count the number of tokens in a text, with a heuristic applied to accelerate long texts\"\"\"\n",
    "    \n",
    "    heuritistic = 6 * max_size\n",
    "\n",
    "    # There is a rare failure case for the below heuristic where superfluous tokens \n",
    "    # may be added from a longer, existing token being split before it was finished.\n",
    "    # e.g. Australia -> 1 token\n",
    "    #      Australi  -> 3 token\n",
    "    #\n",
    "    # We mitigate this failure case by adding the len(longest token)-1 such that\n",
    "    # any ongoing token will be able to finish\n",
    "    #\n",
    "    # Using the cl100k tokenset, the length of the longest non-symbol token is 42\n",
    "    # See: https://gist.github.com/Yardanico/623b3092d0b707119f8c7d90a3596afe\n",
    "    max_token = 42 - 1\n",
    "\n",
    "    if len(text) > heuritistic and counter(text[:heuritistic+max_token]) > max_size:\n",
    "        return max_size + 1\n",
    "    \n",
    "    return counter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6*512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def counter(text: str) -> int:\n",
    "    \"\"\"Count the number of tokens in a text\"\"\"\n",
    "    \n",
    "    if text == '\"' * 5000:\n",
    "        return 1\n",
    "    \n",
    "    return len(text)\n",
    "\n",
    "count('\"' * 5000, 512, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
