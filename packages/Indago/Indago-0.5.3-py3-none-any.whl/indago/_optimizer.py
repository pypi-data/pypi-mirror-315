#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Indago
Python framework for numerical optimization
https://indago.readthedocs.io/
https://pypi.org/project/Indago/

Description: Indago contains several modern methods for real fitness function optimization over a real parameter domain
and supports multiple objectives and constraints. It was developed at the University of Rijeka, Faculty of Engineering.
Authors: Stefan Ivić, Siniša Družeta, Luka Grbčić
Contact: stefan.ivic@riteh.uniri.hr
License: MIT

File content: Definition of top-level Optimizer and Candidate classes.
Usage: from indago import Optimizer, Candidate

"""


import copy
import os.path

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats.qmc import Halton
from datetime import timedelta, datetime
import time
import multiprocessing
import string
from enum import Enum

from rich.console import Console
try:
    from rich.console import Group
except:
    from rich.console import RenderGroup as Group
from rich.progress import Progress, BarColumn, TimeRemainingColumn, TimeElapsedColumn
from rich.live import Live
from rich.table import Table

import indago


class Status(Enum):
    """Enum class for optimization status tracking."""

    NOT_STARTED = 'Optimization not started'
    RUNNING = 'Optimization running'
    RESUMED = 'Optimization running (resumed)'
    FINISHED = 'Optimization finished'
    ERROR = 'Optimization stopped with error'

    def __str__(self):
        return self.name + ': ' + self.value


class Optimizer:
    """Base class for all optimization methods.

    Attributes
    ----------
    dimensions : int
        Number of dimensions of the search space i.e. number of optimization variables.
    evaluation_function : callable
        A function (or a callable class) which takes a ndarray as argument and returns fitness value (float), 
        or in case of multi-objective and/or constrained optimization returns a tuple containing objectives' and
        constraints' values. Optionally, it can take a keyword argument **s** (str) in order to handle a unique string
        generated by the optimizer.
    processes : int or str
        Number of processes in a multiprocessing pool used for parallel evaluation of **evaluation_function**.
        If ``'max'`` all available processor(s) cores will be used. Default: ``1``.
    _pool : Pool
        Private multiprocessing Pool object, used for parallel evaluation.
    objectives : int
        Number of objectives. Default: ``1``.
    objective_weights : list of float
        Weights for objectives used in weighted sum multi-objective optimization. The weighted sum allows combining
        multiple objectives into a single value (fitness) function employed in the optimization. In order to properly
        utilize the weighted sum method, all objectives should be normalized to values of the same order of magnitude.
        This authorizes using several objectives and appointing their importance or priority in the optimization by
        setting the objective weights. By default, all objectives are of same priority, and their values are set to 1.
    objective_labels : list of str
        Names of objectives. Used in outputs such as monitoring, logs, plots etc. Default objective labels are
        ``['obj_0', 'obj_1', ... etc]``.
    constraints : int
        Number of constraints. Default: ``0``.
    constraint_labels : list of str
        Names of constraints. Used in outputs such as monitoring, logs, plots etc. By default, constraint labels are
        set to ``['cnstr_0', 'cnstr_1', ... etc]``.
    max_iterations : int
        Maximum allowed number of iterations of the used optimization method. Default: ``None``.
    max_evaluations : int
        Maximum allowed number of evaluations to be used in the optimization process. Default: ``None``.
        If **max_iterations**, **max_evaluations**, **max_stalled_iterations**, **max_stalled_evaluations**,
        **target_fitness**, and **max_elapsed_time** are all ``None``, defaults to: 50 * **dimensions** ^2.
    max_stalled_iterations : int
        Maximum allowed number of method iterations without any progress. Default: ``None``.
    max_stalled_evaluations : int
        Maximum allowed number of evaluations without any progress. Default: ``None``.
    target_fitness : float
        Target fitness value at which the optimization will be stopped. Default: ``None``.
    max_elapsed_time : float
        Maximum allowed time (in seconds) to be used in the optimization process. Default: ``None``.       
    lb : ndarray or list of float or float
        Lower bounds. If float it will be expanded to ndarray of float of size **dimensions**. If None, defaults to
        -1e100. Values (or members of ndarray or list) not finite (np.inf, np.nan) default to -1e100. If (all members)
        not float, Optimizer.X0 must be provided. Default: ``None``.
    ub : ndarray or list of float or float
        Upper bounds. If float it will be expanded to ndarray of float of size **dimensions**. If None, defaults to
        1e100. Values (or members of ndarray or list) not finite (np.inf, np.nan) default to 1e100. If (all members)
        not float, Optimizer.X0 must be provided. Default: ``None``.
    best : Candidate
        The best candidate state insofar encountered in the optimization process.
    X0 : ndarray or int
        Starting point(s) for the optimization. 1d ndarray, or 2d ndarray with each row representing one design vector.
        If int, represents the number of random candidates generated at the start of optimization. 
        Each method embeds these candidates, fully or partially, in initial population, swarm or starting points. 
        Initial candidates are selected in order of their fitness. 
    _initial_candidates : ndarray
        Private array of CandidateState instances populated and evaluated according to given Optimizer.X0 parameter.
        The array is adopted in each method for establishing initial population, swarm or starting point.
    _evaluated_candidates : ndarray of CandidateState
        Private array used as a buffer of evaluated candidates in current iteration. Enables forwarding a list of all
        evaluated candidates in an interation to be forwarded to post_iteration_processing function even if multiple
        collective evaluations are utilized in a single iteration of a method.
    scatter_method : str
        Method for initializing starting points for the optimization. If ``'random'`` a uniform random generator is used.
        If ``'halton'`` a Halton sequence is used, which will cover the domain more evenly. Default: ``'random'``.
    _halton_sampler : Halton or None
        Private Halton sampler used for generating starting points.
    _seed : int or None
        Private integer for seeding random number generation.
    status : Status
        Optimizer status (Enum).
    _err_msg : str or None
        Private string for storing error message. When an error message is generated, 
        the message is reported and the optimization is aborted.
    history : dict
        Dictionary of appropriately formatted arrays that keeps record of optimization convergence. Using dict
        keys one can access the information for the best candidate in any iteration:
        ``history['eval']`` for number of evaluations (shape ``[iterations, 1]``),
        ``history['X']`` for optimization vector (shape: ``[iterations, dimensions]``),
        ``history['O']`` for objectives (shape ``[iterations, objectives]``),
        ``history['C']`` for constraints (shape ``[iterations, constraints]``), and
        ``history['f']`` for fitness (shape ``[iterations, 1]``. The same data rearranged in a single matrix is stored
        in ``convergence_log_file``.
    monitoring : str
        Adjust the monitoring style and level of output produced during the optimization. ``'none'`` or
        ``None`` suppresses all output. For displaying a line-per-iteration output to the terminal use
        ``'basic'`` which can display a colored output if supported by your terminal. For displaying a
        progress bar and the dynamically updated best solution use ``'dashboard'``. Default: ``'none'``.
    _rich_console : Console
        Private rich Console, used for rich output.
    convergence_log_file : str
        Convergence log file name. The convergence log file is suitably formatted allowing reading
        it using ``numpy.loadtxt``, and it includes rows for: number of iterations, number of evaluations, optimization
        variables, objectives, constraints and fitness. Default: ``None`` (no convergence log is stored).
    evals_db : str
        A file name for storing all evaluated candidates . The evaluation database is stored in Numpy's binary (.npy)
        file format that can be read using ``numpy.load``, and it includes rows for: optimization variables,
        objectives, constraints and fitness. Default: ``None`` (evaluations database is not stored).
    forward_unique_str : bool
        If ``True``, unique string forwarding is used. The strings are generated by Indago and are then 
        passed on to **evaluation_function** as an additional argument. Default: ``False``.
    _unique_str_list : list
        Private list for tracking used unique strings.
    post_iteration_processing : callable
        Handle for a callable object (e.g. function) that is being evoked after each iteration of the method. The
        function is expected to process following arguments ``it`` (``int``, current iteration), ``candidates`` (an
        array of CandidateStates evaluated in the current iteration) and ``best`` (best CandidateState found until
        current iteration).
    safe_evaluation : bool
        If ``True``, in case of failed evaluations exceptions thrown by the **evaluation_function** 
        are caught and ``np.nan`` is returned instead. Default: ``False``.
    eval_fail_count : int
        Used for counting failed evaluations.
    eval_fail_behavior : str
        Controlling behavior in case of **evaluation_function** returning ``np.nan``. 
        If ``'abort'`` the optimization is stopped at the first event of evaluation function returning ``np.nan``.
        If ``'ignore'`` the optimizer will ignore any ``np.nan`` values returned by the evaluation function (not all
        Indago methods support this). If ``'retry'`` the optimizer will try to resolve the issue by repeatedly receding
        a failed design vector a small step towards the best solution thus far. Default: ``'abort'``.
    eval_retry_attempts : int
        Number of retry attempts in case of failed evaluation. Used only if ``eval_fail_behavior='retry'``.
        Default: ``10``.
    eval_retry_recede : float
        Relative distance in the range ``[0, 1]`` for which the failed evaluation vector will be translated in the
        direction of the best solution thus far. Used only if ``eval_fail_behavior='retry'``. Default: ``0.01``.
    it : int
        Used for counting iterations.
    eval : int
        Used for counting evaluations.
    _stalled_it : int
        Used for counting stalled iterations.
    _stalled_eval : int
        Used for counting stalled evaluations.
    elapsed_time : float
        Used for tracking time elapsed during optimization.
    _clock_start : float
        Clock time at the start of the optimization (used for computing elapsed_time).
    params : dict
        Method parameters, in the form of parameter name (dict key as str), parameter value (dict value).
        Defaults specific to the optimization method used.
    
    Returns
    -------
    Optimizer
        Optimizer instance
        
    """

    def __init__(self):
        
        self.variant = None

        self.dimensions = None
        self.evaluation_function = None

        self.processes = 1
        self._pool = None

        self.objectives = 1
        self.objective_weights = None
        self.objective_labels = None
        self.constraints = 0
        self.constraint_labels = None

        self.max_iterations = None
        self.max_evaluations = None
        self.max_stalled_iterations = None
        self.max_stalled_evaluations = None
        self.target_fitness = None
        self.max_elapsed_time = None

        self.lb = None
        self.ub = None

        self.X0 = None
        self.scatter_method = None
        self._seed = None
        self._halton_sampler = None

        self.status = Status.NOT_STARTED
        self._err_msg = None
        self.history = None
        self.monitoring = None
        self.convergence_log_file = None
        self.evals_db = None

        self.forward_unique_str = False
        self.post_iteration_processing = None
        self._unique_str_list = []

        self.safe_evaluation = None
        self.eval_fail_count = 0
        self.eval_fail_behavior = None
        self.eval_retry_attempts = None
        self.eval_retry_recede = None

        self.params = {}

        self._initial_candidates = None
        self._evaluated_candidates = np.array([], dtype=Candidate)
        self.best = None
        self.it = 0
        self.eval = 0
        self._stalled_it = None
        self._stalled_eval = None
        self.elapsed_time = 0
        self._clock_start = None

        # for EEEO
        self._inject = None


    def _progress_factor(self):
        """Private method for calculating progress factor ranging from zero to one,
        based on iterations or evaluations (whichever is running out faster).
        Used in optimization methods which adapt their behavior during the optimization process
        based on the time passed.
        
        Returns
        -------
        progress_factor : float
            A number between zero and one, representing the procedural progress 
            of the optimization process.
            
        """

        if self.it <= 1:
            assert self.max_iterations or self.max_evaluations or self.max_elapsed_time, \
                'optimizer.max_iteration, optimizer.max_evaluations, or self.max_elapsed_time should be provided for this method/variant'

        prog = []
        if self.max_iterations:
            prog.append(self.it / self.max_iterations)
        if self.max_evaluations:
            prog.append(self.eval / self.max_evaluations)
        if self.max_elapsed_time:
            prog.append(self.elapsed_time / self.max_elapsed_time)
        return max(prog)

    def _evaluation_function_safe(self, X, s=None):
        """Private method for wrapping evaluation function in try-except.
        
        Parameters
        ----------
        X : np.array
            Design vector.
        s : str
            Unique string.
            
        Returns
        -------
        fitness : float
            Fitness value as computed by evaluation function, 
            or ``np.nan`` if failed to compute evaluation function.
            
        """

        try:
            if s is None:
                return self.evaluation_function(X)
            else:
                return self.evaluation_function(X, s)
        except:
            if self.objectives == 1 and self.constraints == 0:
                return np.nan
            else:
                return (np.nan,) * (self.objectives + self.constraints)

    def _init_optimizer(self):
        """Private method for Optimizer initialization performed just prior to
        starting the optimization. Checks the types and values of user defined 
        Optimizer attributes. Automatically sets optional Optimizer-level 
        parameters to default values if they are not provided.

        Returns
        -------
        None
            Nothing
            
        """

        # check for deprecated optimizer attributes
        for old_name, new_name in [['method', 'variant'],
                                   ['number_of_processes', 'processes'],
                                   ['iterations', 'max_iterations'],
                                   ['maximum_evaluations', 'max_evaluations'],
                                   ['maximum_stalled_iterations', 'max_stalled_iterations'],
                                   ['maximum_stalled_evaluations', 'max_stalled_evaluations'],
                                   ]:
            assert old_name not in dir(self), \
                f"'optimizer.{old_name}' parameter is deprecated, please use 'optimizer.{new_name}'"

        # check for unknown attributes
        for attr in dir(self):
            if not attr.startswith('_'):
                assert attr in \
                    'variant dimensions evaluation_function processes \
                    objectives objective_weights objective_labels constraints \
                    constraint_labels max_iterations max_evaluations \
                    max_stalled_iterations max_stalled_evaluations target_fitness \
                    max_elapsed_time lb ub best X0 scatter_method history monitoring \
                    convergence_log_file evals_db forward_unique_str \
                    post_iteration_processing safe_evaluation eval_fail_count \
                    eval_fail_behavior eval_retry_attempts eval_retry_recede \
                    it eval elapsed_time params status \
                    methods \
                    optimize plot_history copy'.split(' '), \
                        f"Unknown optimizer attribute '{attr}'"

        # check if some missing attributes are available in evaluation_function
        for attr in 'dimensions lb ub objective_weights objective_labels constraint_labels'.split(' '):
            if getattr(self, attr) is None and hasattr(self.evaluation_function, attr):
                setattr(self, attr, getattr(self.evaluation_function, attr))
        if self.objectives == 1 and hasattr(self.evaluation_function, 'objectives'):
            self.objectives = self.evaluation_function.objectives
        if self.constraints == 0 and hasattr(self.evaluation_function, 'constraints'):
            self.constraints = self.evaluation_function.constraints

        # check for no bounds
        if self.lb is None:
            self.lb = -np.inf
        if self.ub is None:
            self.ub = np.inf
        if not np.isfinite(self.lb).all() or not np.isfinite(self.ub).all():
            assert self.X0 is not None, \
                "(some of the) bounds are not provided or are given as +/-np.inf or np.nan, optimizer.X0 needed"
        self.lb = np.nan_to_num(self.lb, nan=-1e100, posinf=1e100, neginf=-1e100)
        self.ub = np.nan_to_num(self.ub, nan=1e100, posinf=1e100, neginf=-1e100)
            
        # check dimensions or get it from lb/ub
        if self.dimensions is not None:
            assert isinstance(self.dimensions, int) and self.dimensions > 0, \
                "optimizer.dimensions should be positive integer"
        else:
            self.dimensions = max(np.size(self.lb), np.size(self.ub))
            assert self.dimensions > 1, \
                "optimizer.lb and optimizer.ub both of size 1, missing optimizer.dimensions"

        # expand scalar lb/ub
        if np.size(self.lb) == 1:
            self.lb = np.full(self.dimensions, self.lb)
        if np.size(self.ub) == 1:
            self.ub = np.full(self.dimensions, self.ub)

        # in case lb/ub is a list or tuple
        self.lb = np.array(self.lb)
        self.ub = np.array(self.ub)

        assert np.size(self.lb) == np.size(self.ub) == self.dimensions, \
            "optimizer.lb and optimizer.ub should be of equal size or scalar"

        assert (self.lb < self.ub).all(), \
            "optimizer.lb should be strictly lower than optimizer.ub"

        assert callable(self.evaluation_function), \
            "optimizer.evaluation_function should be callable"

        if self.safe_evaluation is None:
            self.safe_evaluation = False
        assert isinstance(self.safe_evaluation, bool), \
            "optimizer.safe_evaluation should be True/False"
        if not self.safe_evaluation:
            self._evaluation_function_safe = self.evaluation_function

        assert isinstance(self.objectives, int) and self.objectives > 0, \
            "optimizer.objectives should be positive integer"

        assert isinstance(self.constraints, int) and self.constraints >= 0, \
            "optimizer.constraints should be non-negative integer"

        if self.objective_weights is None:
            self.objective_weights = np.ones(self.objectives) / self.objectives
        else:
            assert len(self.objective_weights) == self.objectives, \
                "optimizer.objective_weights list should contain number of elements equal to optimizer.objectives"

        if self.objective_labels is None:
            self.objective_labels = [f'obj_{o}' for o in range(self.objectives)]
        else:
            assert len(self.objective_labels) == self.objectives, \
                "optimizer.objective_labels list should contain number of strings equal to optimizer.objectives"

        if self.constraint_labels is None:
            self.constraint_labels = [f'cnstr_{c}' for c in range(self.constraints)]
        else:
            assert len(self.constraint_labels) == self.constraints, \
                "optimizer.constraint_labels list should contain number of strings equal to optimizer.constraints"

        assert (isinstance(self.processes, int) and self.processes > 0) \
               or self.processes == 'max', \
            "optimizer.processes should be positive integer or \'max\'"
        if self.processes == 'max':
            self.processes = multiprocessing.cpu_count()
        if self.processes > 1:
            self._log(f'Preparing a multiprocess pool for {self.processes} processes.')
            try:
                self._pool = multiprocessing.Pool(self.processes)
            except RuntimeError as err:
                assert not "if __name__ == '__main__'" in str(err), \
                    "Parallel evaluation (optimizer.processes > 1) needs Indago optimizers to be started inside \"if __name__ == '__main__':\""
                raise err

        if self.max_iterations:
            assert isinstance(self.max_iterations, int) and self.max_iterations > 0, \
                "optimizer.max_iterations should be positive integer"

        if self.max_evaluations:
            assert isinstance(self.max_evaluations, int) and self.max_evaluations > 0, \
                "optimizer.max_evaluations should be positive integer"

        if self.max_stalled_iterations:
            assert isinstance(self.max_stalled_iterations, int) and self.max_stalled_iterations > 0, \
                "optimizer.max_stalled_iterations should be positive integer"
            self._stalled_it = 0

        if self.max_stalled_evaluations:
            assert isinstance(self.max_stalled_evaluations, int) and self.max_stalled_evaluations > 0, \
                "optimizer.max_stalled_evaluations should be positive integer"
            self._stalled_eval = 0

        if self.max_elapsed_time:
            assert self.max_elapsed_time > 0, \
                "optimizer.max_elapsed_time should be positive number"

        if not (self.max_iterations or self.max_evaluations
                or self.max_stalled_iterations or self.max_stalled_evaluations
                or self.target_fitness or self.max_elapsed_time):
            # self.max_iterations = 100 * self.dimensions
            self.max_evaluations = 50 * self.dimensions ** 2

        if self.eval_fail_behavior:
            assert self.eval_fail_behavior == 'abort' or \
                   self.eval_fail_behavior == 'retry' or \
                   self.eval_fail_behavior == 'ignore', \
                "optimizer.eval_fail_behavior should be 'abort'/'retry'/'ignore'"
        else:
            self.eval_fail_behavior = 'abort'

        if self.eval_retry_attempts:
            assert isinstance(self.eval_retry_attempts, int) and self.eval_retry_attempts > 0, \
                "optimizer.eval_retry_attempts should be positive integer"
        else:
            self.eval_retry_attempts = 10
        if self.eval_retry_recede:
            assert isinstance(self.eval_retry_recede, float) and 0 < self.eval_retry_recede < 1, \
                "optimizer.eval_retry_attempts should be positive number in range (0,1)"
        else:
            self.eval_retry_recede = 0.01

        if self.scatter_method:
            assert self.scatter_method == 'random' or self.scatter_method == 'halton', \
                "optimizer.scatter_method should be 'random' or 'halton'"
        else:
            self.scatter_method = 'random'

        if self.scatter_method == 'halton':
            self._halton_sampler = Halton(self.dimensions, seed=self._seed)

        assert isinstance(self.monitoring, str) or self.monitoring is None, \
            'optimizer.monitoring should be a string or None'

        if isinstance(self.monitoring, str):
            # Convert to lowercase string
            self.monitoring = self.monitoring.lower()
        if self.monitoring is None:
            self.monitoring = 'none'

        if self.monitoring == 'none':
            pass

        elif self.monitoring == 'basic':
            self._rich_console = Console(highlight=False)

        elif self.monitoring == 'dashboard':

            self._progress_bar = Progress(
                "[progress.description]{task.description}",
                # "{task.completed:} of {task.total}",
                BarColumn(),
                "[progress.percentage]{task.percentage:>3.0f}%",
                # "Elapsed: {task.time_remaining}",
                "Elapsed:",
                TimeElapsedColumn(),
                "Remaining:",
                TimeRemainingColumn(),
                # expand=True,
                )

            self._progress_bar_tasks = []

            if self.max_iterations:
                self._progress_bar_tasks.append(self._progress_bar.add_task("Iterations:",
                                                                              total=self.max_iterations))
            if self.max_stalled_iterations:
                self._progress_bar_tasks.append(self._progress_bar.add_task("Stalled iterations:",
                                                                              total=self.max_stalled_iterations))
            if self.max_evaluations:
                self._progress_bar_tasks.append(self._progress_bar.add_task("Evaluations:",
                                                                              total=self.max_evaluations))
            if self.max_stalled_evaluations:
                self._progress_bar_tasks.append(self._progress_bar.add_task("Stalled evaluations:",
                                                                              total=self.max_stalled_evaluations))
            if self.target_fitness:
                self._progress_bar_tasks.append(self._progress_bar.add_task("Target fitness:", total=1))

            if self.max_elapsed_time:
                self._progress_bar_tasks.append(self._progress_bar.add_task("Elapsed time:",
                                                                              total=self.max_elapsed_time))

            self._live = Live(self._update_progress_bar())

        else:
            assert False, f'Unknown value for optimizer.monitoring parameter ({self.monitoring})'

        # Record clock time
        self._clock_start = time.time()

        # Initialize history
        self.history = {'eval': np.empty([0, 1]),
                        'X': np.empty([0, self.dimensions]),
                        'O': np.empty([0, self.objectives]),
                        'C': np.empty([0, self.constraints]),
                        'f': np.empty([0, 1]),
                        }
        self._init_convergence_log()
        
    def _update_progress_bar(self):
        """
        Private function for updating rich progress bar for 'dashboard' monitoring.

        Returns
        -------
        None
            Nothing

        """

        progress = []
        report_str = ''

        if self.max_iterations:
            progress.append(self.it)
            report_str += f'\nCurrent iteration: {self.it}'

        if self.max_stalled_iterations:
            progress.append(self._stalled_it)
            report_str += f'\nStalled iterations: {self._stalled_it}'

        if self.max_evaluations:
            progress.append(self.eval)
            report_str += f'\nTotal evaluations: {self.eval}'

        if self.max_stalled_evaluations:
            progress.append(self._stalled_eval)
            report_str += f'\nStalled evaluations: {self._stalled_eval}'
            # This is related to the Issue #29 reported in GitLab
            # self._log(f'{self._stalled_eval}, {self.max_stalled_evaluations}')
            # report_str += f'\nStalled evaluations: {np.min([self._stalled_eval, self.max_stalled_evaluations])}'

        if self.target_fitness and self.best:
            f0 = None
            # for h in self.results.cHistory:
            #     if np.all(h[2].C <= 0):
            #         f0 = h[2].f
            #         break
            # print(f'{f0=}')
            for i in range(self.it + 1):
                if np.all(self.history['C'][i, :] <= 0):
                    f0 = self.history['f'][i, 0]
                    break
            if f0:
                if self.best.f > self.target_fitness:
                    p = (self.best.f - self.target_fitness) / (f0 - self.target_fitness)
                else:
                    p = 0
                p = 1 - np.log10(1 + 9 * p)
            else:
                p = 0
            progress.append(p)

        if self.max_elapsed_time:
            progress.append(self.elapsed_time)
            report_str += f'\nElapsed time: {timedelta(seconds=self.elapsed_time)}'

        for task, prog in zip(self._progress_bar_tasks, progress):
            self._progress_bar.update(task, completed=prog)

        best = 'None'
        if self.best is not None:
            best = str(self.best.X)
            label_len = max([len(lbl) for lbl in \
                             (self.objective_labels + self.constraint_labels)]) + 2
            for o, o_label in zip(self.best.O, self.objective_labels):
                best += '\n[magenta]' + f'{o_label}: '.rjust(label_len) + f'{o:12.5e} [/magenta]'
            for c, c_label in zip(self.best.C, self.constraint_labels):
                if c <= 0:
                    best += '\n[green]' + f'{c_label}: '.rjust(
                        label_len) + f'{c:12.5e}   => (satisfied)[/green]'
                else:
                    best += '\n[red]' + f'{c_label}: '.rjust(label_len) + f'{c:12.5e}   => (violated)[/red]'
            best += '\n' + 'Fitness: '.rjust(label_len) + f'{self.best.f:12.5e}' + '\n'
        report_str += f'\nBest:\n{best}'

        return Group(f'[b]Indago[/b] {indago.__version__}',
                     f'{self.__class__.__name__} optimization running\n',
                     self._progress_bar,
                     report_str)
        
    def _initialize_X(self, candidates):
        """Private method for generating initial positions of given candidates, by using the appropriate method
        (Optimizer.scatter_method).

        Parameters
        ----------
        candidates : list of Candidate
            Candidates whose X must be initialized.

        Returns
        -------
        None
            Nothing
        """
        
        if self.scatter_method == 'random':
            for c in candidates:
                c.X = np.random.uniform(size=self.dimensions, low=self.lb, high=self.ub)

        if self.scatter_method == 'halton':
            for c in candidates:
                c.X = self.lb + self._halton_sampler.random(n=1)[0] * (self.ub - self.lb)

    def _evaluate_initial_candidates(self):
        """Private method for evaluating initial candidates. This method populates 
        and evaluates private list of initial candidates (Optimizer._initial_candidates)
        according to provided initial points Optimizer.X0.

        Returns
        -------
        None
            Nothing

        """

        if self.X0 is not None:
            assert isinstance(self.X0, np.ndarray) or isinstance(self.X0, int), \
                "optimizer.X0 should be 1d or 2d np.array, or integer"
            if isinstance(self.X0, np.ndarray):
                assert self.X0.ndim == 1 or self.X0.ndim == 2, \
                    "optimizer.X0 should be 1d or 2d np.array"
                # if 1D convert to 2D
                if self.X0.ndim == 1:
                    self.X0 = np.array([self.X0])
                self._initial_candidates = np.array([Candidate(optimizer=self) for i in range(self.X0.shape[0])])
                for i, cs in enumerate(self._initial_candidates):
                    cs.X = self.X0[i, :]

            if isinstance(self.X0, int):
                assert self.X0 > 0, \
                    "optimizer.X0 should be a positive integer"
                n = self.X0
                self._initial_candidates = np.array([Candidate(optimizer=self) for i in range(n)])
                # for cs in self._initial_candidates:
                #     cs.X = np.random.uniform(self.lb, self.ub, self.dimensions)
                self._initialize_X(self._initial_candidates)

            self._collective_evaluation(self._initial_candidates)
            self._initial_candidates = np.sort(self._initial_candidates)

        else:
            self.best = None

    def _init_convergence_log(self):
        """Private method for initializing convergence log. It creates log file and writes optimization setup summary
        and header row.

        Returns
        -------
        None
            Nothing
            
        """

        # Initialize convergence log file
        if self.convergence_log_file:
            if os.path.exists(self.convergence_log_file):
                os.remove(self.convergence_log_file)

            summary = ['#']
            summary.append(f'#  Indago {type(self).__name__} Optimizer')
            summary.append('#')
            for var, value in vars(self).items():
                if not var.startswith('_'):
                    if var in ['it', 'eval', 'history']:
                        continue
                    if isinstance(value, (int, float, str, bool)):
                        summary.append(f'#  {var:25}: {str(value)}')
                    if isinstance(value, (list, dict)) and len(value) > 0:
                        summary.append(f'#  {var:25}: {str(value)}')
                    if isinstance(value, np.ndarray) and np.size(value) > 0:
                        if isinstance(value[0], (int, float)):
                            summary.append(f'#  {var:25}: {np.array_str(value, max_line_width=np.inf)}')
            # for k, v in self.params.items():
            #     summary.append(f'#  {k:25}: {v}')

            summary.append('#')
            summary.append(f'#  Optimization started at {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
            summary.append('#')

            # Columns header
            line = 'Iterations'.rjust(12)
            line += 'Evaluations'.rjust(15)
            line += ''.join([f'x{i + 1}'.rjust(20) for i in range(self.dimensions)])
            for ol in self.objective_labels:
                line += f'{ol}'[:19].strip().rjust(20)
            for cl in self.constraint_labels:
                line += f'{cl}'[:19].strip().rjust(20)
            line += 'Fitness'.rjust(20)

            log_file = open(self.convergence_log_file, 'w')
            log_file.write('\n'.join(summary) + '\n')
            log_file.write('#' + line + '\n')
            log_file.write('#' + '-' * len(line) + '\n')
            log_file.close()

    def _check_params(self, mandatory_params, optional_params, defined_params):
        """Private method which checks if optimizer parameters are defined in Optimizer.params dict.
        Should be called in initializations of derived Optimizer classes. It asserts if any of 
        mandatory parameters is missing and prints a warning if unknown/excessive parameter is provided.

        Parameters
        ----------
        mandatory_params : list of str
            A list of mandatory parameter names/keys for given method.
        optional_params : list of str
            A list of optional parameter names/keys for given method.
        defined_params : list of str
            A list of method's parameters names/keys defined in Optimizer.params dict.

        Returns
        -------
        None
            Nothing
            
        """

        for param in mandatory_params:
            assert param in defined_params, f'Missing parameter {param}'

        for param in defined_params:
            assert param in mandatory_params or param in optional_params, \
                f'Warning: Excessive parameter {param}'

    def __str__(self):
        """Method for a useful printout of optimizer properties. 
        
        Returns
        -------
        printout : str
            String of the fancy table of optimizer properties.
            
        """

        table = Table(title=f'Indago {type(self).__name__} Optimizer')

        table.add_column('Property', justify='left', style='magenta')
        table.add_column('Value', justify='left', style='cyan')

        for var, value in vars(self).items():
            if var in ['history']:
                continue
            if not var.startswith('_'):
                if isinstance(value, (int, float, str, bool, Status)):
                    table.add_row(var, str(value))
                if isinstance(value, (list, dict)) and len(value) > 0:
                    table.add_row(var, str(value))
                if isinstance(value, np.ndarray) and np.size(value) > 0:
                    if isinstance(value[0], (int, float)):
                        table.add_row(var, np.array_str(value, max_line_width=np.inf))

        # the following are important, explicitly adding them to printout
        if self.best:
            table.add_row('best.X', np.array_str(self.best.X, max_line_width=np.inf), style='bold')
            table.add_row('best.f', str(self.best.f), style='bold')

        Console().print(table)
        return ''

    def _log(self, msg, indent=0):
        """Utility function for consistently displaying messages in the output.

        Parameters
        ----------
        msg : str
            Message to be displayed in the output.
        indent : int
            Number of characters used for indentation of the message. Default is 0.
        
        Returns
        -------
        None
            Nothing
            
        """

        if self.monitoring == 'basic':
            print(' ' * indent + msg)
        if self.convergence_log_file:
            log_file = open(self.convergence_log_file, 'a')
            log_file.write(f'#  {msg}\n')
            log_file.close()

    def _progress_log(self):
        """Private method used to produce and print a line for each iteration.
        
        Returns
        -------
        None
            Nothing
            
        """
        
        if self.monitoring == 'basic':
            
            line = f'iter: {self.it:10d}'
            if self.max_iterations:
                line += f'/{self.max_iterations}'
            line += f', eval: {self.eval:10d}'
            if self.max_evaluations:
                line += f'/{self.max_evaluations}'
            for io, ol in enumerate(self.objective_labels):
                line += f', {ol}: [cyan]{self.best.O[io]}[/cyan]'
            for cl, cv in zip(self.constraint_labels, self.best.C):
                if cv > 0:
                    line += f', {cl}: [green bold]{cv}[/green bold]'
                else:
                    line += f', {cl}: [green bold]{cv}[/green bold]'
            line += f', fit: [magenta bold]{self.best.f}[/magenta bold]'
            self._rich_console.print(line)
            # self._log(line)

    def _convergence_log_line(self):
        """Private method used to produce and write a line to 
        convergence log file for each iteration.
        
        Returns
        -------
        None
            Nothing
            
        """

        if not self.convergence_log_file:
            return

        line = f'{self.it}'.rjust(13)
        line += f'{self.eval}'.rjust(15)
        line += ''.join([f'{x:18.12e}'.rjust(20) for x in self.best.X])
        for o in self.best.O:
            line += f'{o:18.12e}'.rjust(20)
        for c in self.best.C:
            line += f'{c:18.12e}'.rjust(20)
        line += f'{self.best.f:18.12e}'.rjust(20)
        line += '\n'

        log_file = open(self.convergence_log_file, 'a')
        log_file.write(line)
        log_file.close()

    def _gen_unique_str(self):
        """Private method for generating a unique string of 16 characters.
        This string is then used when forwarding a unique string to the 
        evaluation function is enabled (Optimizer.forward_unique_str = True). 
        The generated string is guaranteed to be unique within an optimizer.

        Returns
        -------
        s : str
            A unique string.
            
        """

        while True:
            _s = ''.join(np.random.choice([c for c in string.ascii_lowercase]) for _ in range(16))
            if _s not in self._unique_str_list:
                self._unique_str_list.append(_s)
                return _s

    def optimize(self, resume=False, inject=None, seed=None):
        """Method which starts the optimization. The method wraps ``_run`` method 
        of the optimizer's subclass.

        Parameters
        ----------
        resume : bool
            If ``True``, the user is intentionally running the optimizer instance
            which has been run earlier. Otherwise, the optimizer is re-initialized.
        inject : Candidate or its subclass object
            Candidate solution to be injected into the optimizer population.
        seed : int or None
            Random seed. Provide the same value for reproducing identical 
            stochastic procedures.

        Returns
        -------
        optimum : Candidate
            The best solution found in the optimization.
            
        """

        if not resume:
            self.best = None

            self._initial_candidates = None
            self._evaluated_candidates = np.array([], dtype=Candidate)

            self.status = Status.RUNNING
            self._err_msg = None
            self.history = None

            self._unique_str_list = []

            self.eval_fail_count = 0

            self.it = 0
            self.eval = 0
            self._stalled_it = None
            self._stalled_eval = None
            self.elapsed_time = 0
            self._clock_start = None

            np.random.seed(seed)
            self._seed = seed

            self._init_optimizer()

        else:  # Resume
            self._log(f'Optimization resumed at {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
            self._log('')
            self.status = Status.RESUMED

            # For EEEO
            if inject:
                assert hasattr(inject, 'X') and hasattr(inject, 'O') \
                       and hasattr(inject, 'C') and hasattr(inject, 'f'), \
                    'Object given in inject appears not of Candidate (sub)class'
                self._inject = inject

        if self.monitoring == 'dashboard':
            with self._live:
                r = self._run()
        else:
            r = self._run()

        if self.convergence_log_file:
            summary = ['#', f'#  Optimization finished at {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}', '#\n']
            log_file = open(self.convergence_log_file, 'a')
            log_file.write('\n'.join(summary))
            log_file.close()

        # Terminate multiprocessing pool if used
        if hasattr(self, '_pool'):
            if self._pool is not None:
                self._pool.terminate()

        # Cast best candidate to Candidate class
        r_c = Candidate(self)
        r_c.X, r_c.O, r_c.C, r_c.f = r.X, r.O, r.C, r.f

        return r_c

    def _stopping_criteria(self):
        """Private method used to evaluate all specified stopping conditions.

        Returns
        -------
        stop : bool
            ``True`` or ``False``, whether the optimization should stop.
            
        """
        
        stop = False
        status_str = ''

        # Stop if maximum number of iterations reached
        if self.max_iterations:
            if self.it >= self.max_iterations:
                self.status = Status.FINISHED
                status_str = f'{Status.FINISHED.value}: maximum number of iterations reached ({self.max_iterations}).'
                stop = True

        # Stop if maximum number of evaluations surpassed
        if self.max_evaluations:
            if self.eval >= self.max_evaluations:
                self.status = Status.FINISHED
                status_str = f'{Status.FINISHED.value}: maximum number of evaluations reached ({self.eval}).'
                stop = True

        # Stop if fitness threshold achieved
        if self.target_fitness:
            if self.best.f < self.target_fitness and np.all(self.best.C <= 0):
                self.status = Status.FINISHED
                status_str = f'{Status.FINISHED.value}: target fitness achieved ({self.best.f:.6e}).'
                stop = True

        # Stop if maximum number of stalled iterations surpassed
        if self.max_stalled_iterations:
            f_hist = self.history['f'].flatten()
            # f_hist = np.array([r[2].f for r in self.results.cHistory])
            self._stalled_it = np.size(np.where(f_hist == f_hist[-1])) - 1
            if self._stalled_it >= self.max_stalled_iterations:
                self.status = Status.FINISHED
                status_str = f'{Status.FINISHED.value}: maximum stalled iterations reached ({self._stalled_it:d}).'
                stop = True

        # Stop if maximum number of stalled evaluations surpassed
        if self.max_stalled_evaluations:
            f_hist = self.history['f'].flatten()
            # f_hist = np.array([r[2].f for r in self.results.cHistory])
            stalled_it = np.size(np.where(f_hist == f_hist[-1])) - 1
            # print(f'{stalled_it=}, {self.it=}')
            if stalled_it > self.it:
                self._stalled_eval = self.eval
            else:
                self._stalled_eval = int(self.eval - self.history['eval'][-stalled_it - 1][0])
            # print(f'new: {self.stalled_eval=}')
            # self.stalled_eval = self.eval - self.results.cHistory[-stalled_it][1]
            # print(f'old: {self.stalled_eval=}')
            if self._stalled_eval >= self.max_stalled_evaluations:
                self.status = Status.FINISHED
                status_str = f'{Status.FINISHED.value}: maximum stalled evaluations reached ({self._stalled_eval:d}).'
                stop = True

        # Stop if maximum elapsed time reached
        if self.max_elapsed_time:
            if self.elapsed_time >= self.max_elapsed_time:
                self.status = Status.FINISHED
                status_str = (f'{Status.FINISHED.value}: maximum elapsed time reached '
                              f'({timedelta(seconds=self.max_elapsed_time)} s).')
                stop = True

        if stop:
            if self.constraints > 0:
                if self.best.is_feasible():
                    status_str += ' Feasible solution found.'
                else:
                    status_str += ' No feasible solutions found.'
            self._log(status_str)

        if self.monitoring == 'dashboard':
            self._live.update(self._update_progress_bar())

        return stop

    def _finalize_iteration(self):
        """Private method used to perform all administrative tasks at the end 
        of a method's iteration.
    
        Returns
        -------
        stop : bool
            ``True`` or ``False``, whether the optimization should stop or not.
            
        """

        if self._err_msg is not None:
            self.status = Status.ERROR
            status_str = f'{Status.ERROR.value}: {self._err_msg}'
            self._log(status_str)
            assert False, \
                f'{self._err_msg}. OPTIMIZATION ABORTED'

        self._progress_log()

        if self.convergence_log_file:
            self._convergence_log_line()

        if self.evals_db:
            cols = self.dimensions + self.objectives + self.constraints + 1
            M = np.full([self._evaluated_candidates.size, cols], np.nan)
            for i, c in enumerate(self._evaluated_candidates):
                M[i, :] = np.hstack([c.X, c.O, c.C, c.f])

            with open(self.evals_db, 'ab') as f:
                np.save(f, M)

        # Post iterational processing
        if self.post_iteration_processing:
            _candidates = np.sort(self._evaluated_candidates, kind='stable')
            self.post_iteration_processing(self.it, _candidates, self.best)

        # Flushing evaluated candidates
        self._evaluated_candidates = np.array([], dtype=Candidate)

        # Tracking time
        self.elapsed_time = time.time() - self._clock_start

        # Checking stopping criteria
        if self._stopping_criteria():
            return True
        else:
            # Counting iterations
            self.it += 1
            return False

    def _multiprocess_evaluate(self, candidates):
        """Private method used for calling parallel evaluation of multiple candidates. 
        It relies on multiprocessing pool using map and starmap methods. The objectives, 
        constraints and fitness of candidates in the list are updated after the evaluation.

        Parameters
        ----------
        candidates : list of Candidate
            A list of candidates to be evaluated.
            
        Returns
        -------
        None
            Nothing
            
        """

        if self.forward_unique_str:
            for p in range(len(candidates)):
                candidates[p].unique_str = self._gen_unique_str()
            results = self._pool.starmap(self._evaluation_function_safe,
                                         [(c.X, c.unique_str) for c in candidates])
        else:
            results = self._pool.map(self._evaluation_function_safe, 
                                     [c.X for c in candidates])

        # if failed evaluation
        if np.any(np.isnan(np.array(results))):
            self.eval_fail_count += int(np.sum(np.isnan(np.array(results))) \
                                        / (self.objectives + self.constraints))

            if self.eval_fail_behavior == 'ignore':
                # report to log evaluation failed
                self._log('evaluation function failed, ignoring')

            elif self.eval_fail_behavior == 'abort':
                # report to log evaluation failed
                self._log('evaluation function failed, aborting optimization')
                self._err_msg = 'EVALUATION FAILED'
                return

            elif self.eval_fail_behavior == 'retry':
                nans_index = np.nonzero(np.isnan(np.array(results).flatten()))[0]
                nans_index = nans_index[::(self.objectives + self.constraints)]
                nans_index //= (self.objectives + self.constraints)
                for p in nans_index:
                    for i in range(self.eval_retry_attempts):
                        self._log(f'evaluation function failed, attempting retry #{i + 1}')
                        
                        if self.best:
                            goodX = self.best.X
                        else:
                            goodX = (self.lb + self.ub) / 2
                    
                        candidates[p].X += self.eval_retry_recede * \
                                           (goodX - candidates[p].X)
                        # re-evaluate
                        result_p = self._evaluation_function_safe(candidates[p].X)

                        if not np.any(np.isnan(result_p)):
                            # if successful, transfer fitness info back to results
                            results[p] = result_p
                            break
                        else:
                            self.eval_fail_count += 1
                    
                    else:
                        if self.eval_retry_attempts > 0:
                            # report to log retrying exhausted
                            self._log('evaluation function retry attempts exhausted, aborting optimization')
                            self._err_msg = 'TOO MANY FAILED EVALUATIONS'
                            return

        if self.objectives == 1 and self.constraints == 0:
            # Fast evaluation
            for p in range(len(candidates)):
                candidates[p].f = results[p]
                candidates[p].O[0] = results[p]
        else:
            # Full evaluation
            for p in range(len(candidates)):
                for io in range(self.objectives):
                    candidates[p].O[io] = results[p][io]
                candidates[p].f = np.dot(candidates[p].O, self.objective_weights)
                for ic in range(self.constraints):
                    candidates[p].C[ic] = results[p][self.objectives + ic]

    def _collective_evaluation(self, candidates):
        """Private function used for evaluation of multiple candidates which 
        automatically conducts parallel or serial evaluation and forwards 
        a unique string to the evaluation function. Evaluation is performed 
        in-place and the candidates provided as argument are updated.

        Parameters
        ----------
        candidates : list of Candidate
            A list of candidates to be evaluated.
            
        Returns
        -------
        None
            Nothing
            
        """

        n = len(candidates)
        self.eval += n

        if self.processes > 1:
            self._multiprocess_evaluate(candidates)

        else:
            for p in range(n):

                if self.forward_unique_str:
                    candidates[p].unique_str = self._gen_unique_str()
                    result = self._evaluation_function_safe(candidates[p].X, candidates[p].unique_str)
                else:
                    result = self._evaluation_function_safe(candidates[p].X)

                if self.objectives == 1 and self.constraints == 0:
                    # Fast evaluation
                    candidates[p].f = result
                    candidates[p].O[0] = result
                else:
                    # Full evaluation
                    for io in range(self.objectives):
                        candidates[p].O[io] = result[io]
                    candidates[p].f = np.dot(candidates[p].O, self.objective_weights)
                    for ic in range(self.constraints):
                        candidates[p].C[ic] = result[self.objectives + ic]
                
                # if failed evaluation
                if np.isnan(candidates[p].f):
                    self.eval_fail_count += 1

                    if self.eval_fail_behavior == 'ignore':
                        # report to log evaluation failed
                        self._log('evaluation function failed, ignoring')

                    elif self.eval_fail_behavior == 'abort':
                        # report to log evaluation failed
                        self._log('evaluation function failed, aborting optimization')
                        self._err_msg = 'EVALUATION FAILED'
                        return

                    elif self.eval_fail_behavior == 'retry':
                        for i in range(self.eval_retry_attempts):
                            self._log(f'evaluation function failed, attempting retry #{i + 1}')
                            
                            if self.best:
                                goodX = self.best.X
                            else:
                                goodX = (self.lb + self.ub) / 2
                            
                            candidates[p].X += self.eval_retry_recede * \
                                               (goodX - candidates[p].X)
                            # re-evaluate
                            result = self._evaluation_function_safe(candidates[p].X)
                            if self.objectives == 1 and self.constraints == 0:
                                # Fast evaluation
                                candidates[p].f = result
                                candidates[p].O[0] = result
                            else:
                                # Full evaluation
                                for io in range(self.objectives):
                                    candidates[p].O[io] = result[io]
                                candidates[p].f = np.dot(candidates[p].O, self.objective_weights)
                                for ic in range(self.constraints):
                                    candidates[p].C[ic] = result[self.objectives + ic]

                            if not np.isnan(candidates[p].f):
                                break
                            else:
                                self.eval_fail_count += 1
            
                        else:
                            if self.eval_retry_attempts > 0:
                                # report to log retrying exhausted
                                self._log('evaluation function retry attempts exhausted, aborting optimization')
                                self._err_msg = 'TOO MANY FAILED EVALUATIONS'
                                return

        # print(f'it {self.it:3d}: ' + ', '.join([f'{c.f:.6e}' for c in candidates]))
        # Expand list of candidates evaluated in current iteration
        self._evaluated_candidates = np.append(self._evaluated_candidates, np.array([c.copy() for c in candidates]))

        # Determine new best candidate state
        if self.best is None:
            self.best = np.min(candidates).copy()
        else:
            # self.best = np.min(np.append(candidates, [self.best])).copy()
            candidates_best = np.sort(candidates, kind='stable')[0]
            if candidates_best.f == self.best.f:
                if np.any(candidates_best.X != self.best.X):
                    self._log(f'Warning: nonunique optimum; multiple best candidates with same fitness but different X: [{", ".join(f"{x:18.12e}" for x in candidates_best.X)}]')
            if candidates_best <= self.best:
                self.best = candidates_best.copy()

        # Record to history
        self._update_history()

        if self.monitoring == 'dashboard':
            self._live.update(self._update_progress_bar())

    def _update_history(self):
        """Private method that updates Optimizer.history dict entries according 
        to current Optimizer.best Candidate.

        Returns
        -------
        None
            Nothing
            
        """

        if self.history['eval'].size < self.it + 1:
            self.history['eval'] = np.vstack([self.history['eval'], self.eval])
            self.history['X'] = np.vstack([self.history['X'], self.best.X])
            self.history['O'] = np.vstack([self.history['O'], self.best.O])
            self.history['C'] = np.vstack([self.history['C'], self.best.C])
            self.history['f'] = np.vstack([self.history['f'], self.best.f])
        else:
            self.history['eval'][-1] = self.eval
            self.history['X'][-1] = self.best.X
            self.history['O'][-1] = self.best.O
            self.history['C'][-1] = self.best.C
            self.history['f'][-1] = self.best.f

    def plot_history(self, filename=None, title=None):
        """Plot the convergence of optimization variables, objectives, constraints and fitness.

        Parameters
        ----------
        filename : str
            Saves the convergence figure according to provided filename and closes the figure.
        title : str or None
            Optional title of the figure. If None (default), the title is automatically generated.
        
        Returns
        -------
        None or (fig, axes)
            If filename is specified nothing (None) is returned, otherwise the tuple of figure and axis array is
            returned.
        """

        if title is None:
            title = f'Indago {self.__class__.__name__} optimization convergence'

        fig, axes = plt.subplots(figsize=(12, 10), nrows=4 if self.constraints > 0 else 3, constrained_layout=True, sharex=True)
        if self.constraints > 0:
            ax_x, ax_o, ax_c, ax_f = axes
        else:
            ax_x, ax_o, ax_f = axes
            ax_c = None

        E = self.history['eval'][:, 0]
        lw = 1 if self.dimensions <= 10 else 0.5

        # Plot optimization variables
        if ax_x:
            for i in range(self.dimensions):
                X = (self.ub[i] - self.history['X'][:, i]) / (self.ub[i] - self.lb[i])
                ax_x.plot(E, X, lw=lw, ls='-',
                          label=f'$x_{{{i + 1}}}$' if self.dimensions < 15 else None)
            ax_x.set_ylabel('$\mathbf{Optimization~variables}$\n(norm lin scale)')
            ax_x.set_yticks([0, 1], 'LB UB'.split())
            ax_x.set_xlim(E.min(), E.max())
            ax_x.set_ylim(0, 1)
            ax_x.set_facecolor("snow")
            ax_x.spines['right left'.split()].set_visible(False)
            ax_x.spines['top bottom'.split()].set_linewidth(0.5)
            ax_x.spines['top bottom'.split()].set_zorder(-1)

            if self.dimensions <= 10:
                legend = ax_x.legend(ncol=12, fontsize='small',
                                     loc = 'upper center',
                                     bbox_to_anchor = (0.5, 1.15),
                                     )
                frame = legend.get_frame()
                frame.set_facecolor('none')
                frame.set_edgecolor('none')

        # Plot objectives
        if ax_o:

            # if self.objectives > 1:
            for i in range(self.objectives):
                O = self.history['O'][:, i]
                # O = 100 * (self.history['O'][:, i] - f_min) / (f_max - f_min)
                ax_o.plot(E, O, lw=1, label=self.objective_labels[i])

            ax_o.set_ylabel('$\mathbf{Objectives}$\n(lin scale)')
            ax_o.set_xlim(E.min(), E.max())
            # ax_o.set_ylim(0, 1)
            ax_o.set_facecolor("snow")
            ax_o.spines['right left'.split()].set_visible(False)
            ax_o.spines['top bottom'.split()].set_linewidth(0.5)
            ax_o.spines['top bottom'.split()].set_zorder(-1)

            if self.objectives <= 10:
                legend = ax_o.legend(ncol=10, fontsize='small',
                                     loc = 'upper center',
                                     bbox_to_anchor = (0.5, 1.15),
                                     )
                frame = legend.get_frame()
                frame.set_facecolor('none')
                frame.set_edgecolor('none')


        # Plot constraints
        if ax_c:

            for ic in range(self.constraints):
                C = self.history['C'][:, ic]

                i_feasible = E.size
                for i in range(C.size):
                    if C[i] <= 0:
                        i_feasible = i
                        break

                ax_c.plot(E, C, lw=1, ls='-', label=self.constraint_labels[ic])
                ax_c.plot(E[:i_feasible], C[:i_feasible], ls='-', dashes=(1.5, 0.8), lw=2, c='orangered',ms=3)

            ax_c.plot([], [], ls=':', lw=2, c='r', label='Unfeasible')
            ax_c.set_ylabel('$\mathbf{Constraints}$\n(sym log scale)')
            ax_c.set_xlim(E.min(), E.max())
            # ax_o.set_ylim(0, 1)
            ord = 10 ** np.ceil(np.log10(np.max(np.abs(self.history['C']))))
            ax_c.set_yscale('symlog', linthresh=1e-3*ord)
            ax_c.set_facecolor("snow")
            ax_c.spines['right left'.split()].set_visible(False)
            ax_c.spines['top bottom'.split()].set_linewidth(0.5)
            ax_c.spines['top bottom'.split()].set_zorder(-1)
            ax_c.axhline(lw=1, c='k', ls='-')

            if self.constraints <= 10:
                legend = ax_c.legend(ncol=11, fontsize='small',
                                     loc = 'upper center',
                                     bbox_to_anchor = (0.5, 1.15),
                                     )
                frame = legend.get_frame()
                frame.set_facecolor('none')
                frame.set_edgecolor('none')

        # Plot fitness
        if ax_f:
            ax_f.set_xlabel('Evaluations')
            ax_f.set_ylabel('$\mathbf{Fitness}$\n(norm log scale)')
            ax_f.set_facecolor("snow")
            ax_f.spines['right left'.split()].set_visible(False)
            ax_f.spines['top bottom'.split()].set_linewidth(0.5)
            ax_f.spines['top bottom'.split()].set_zorder(-1)

            f_min = np.min(self.history['f'][:, 0])
            f_max = np.max(self.history['f'][:, 0])
            F = self.history['f'][:, 0]

            if f_max > f_min:
                Flin = 99 * (F - f_min) / (f_max - f_min) + 1
            else:
                Flin = np.zeros_like (F)

            ax_f.plot(E, Flin, lw=1.5, c='k', label='Fitness')

            ax_f.set_yscale('symlog')
            f_lim = ax_f.get_ylim()[0], ax_f.get_ylim()[1]
            ax_f.set_ylim(f_lim)

            for ic in range(self.constraints):

                i_feasible = E.size
                for i in range(self.it + 1):
                    if self.history['C'][i, ic] <= 0:
                        i_feasible = i
                        break

                alpha = 0.1 + 0.5/self.constraints
                ax_f.fill_between(E[:i_feasible], f_lim[0], f_lim[1],
                                  color='red', alpha=alpha,
                                  edgecolors='none', label='Unfeasible' if ic == 0 else None)

            for e in np.append(ax_f.get_xticks(), E[-1]):
                if e < E.min() or e > E.max():
                    continue

                props = dict(boxstyle='square,pad=0.3', facecolor='khaki', alpha=0.6,
                             edgecolor='k' if e == E.max() else 'none', lw=0.5)
                i_tick = np.argmin(np.abs(E - e))
                ax_f.plot(e, Flin[i_tick], 'o', c='limegreen' if e == E.max() else 'dodgerblue', ms=3, zorder=10)

                x_offset = 12 if e < 0.8 * E.max() else -25
                y_offset = 12 if e < E.max() else 30
                if Flin[i_tick] > 40:
                    y_offset *= -1
                ax_f.annotate(f'{F[i_tick]:.3e}',
                              (e, Flin[i_tick]),
                              xytext=(x_offset,
                                      y_offset),
                              va='center',
                              ha='center',
                              textcoords='offset points',
                              fontsize='x-small',
                              weight = 'bold' if e == E.max() else 'normal',
                              c='k', # if e < E.max() else 'g',
                              bbox=props,
                              arrowprops=dict(
                                  arrowstyle='-', connectionstyle="arc3,rad=0.05",
                                  color="grey", shrinkA=0, shrinkB=0,
                                  linewidth=0.8,
                              )
                              )

            ax_f.yaxis.set_ticks_position('none')
            ax_f.set_yticks([])

            legend = ax_f.legend(ncol=2, fontsize='small',
                                 loc = 'upper center',
                                 bbox_to_anchor = (0.5, 1.15),
                                 )
            frame = legend.get_frame()
            frame.set_facecolor('none')
            frame.set_edgecolor('none')

        for ax in axes:
            ax.grid(axis='x', color='grey', lw=0.1, ls='--')
            ax.grid(axis='y', color='grey', lw=0.1, ls='--')
            ax.tick_params(axis='both', which='major', labelsize='small')
            # for tick in ax.xaxis.get_major_ticks():
            #     tick.label.set_fontsize('small')
            # for tick in ax.yaxis.get_major_ticks():
            #     tick.label.set_fontsize('small')

        fig.align_ylabels(axes)
        fig.suptitle(title, fontsize='large', c='k', weight='bold')
        if filename:
            plt.savefig(filename, dpi=150)
            plt.close(fig)
        else:
            return fig, axes
    
    def copy(self):
        """Method for creating a true (deep) copy of the Optimizer. 
        
        Returns
        -------
        opt
            Optimizer instance.
            
        """

        opt = copy.deepcopy(self)
        opt._pool = copy.deepcopy(self._pool)
        return opt
    
    # these two functions are needed for multiprocessing to work when using monitoring
    def __getstate__(self):
        self_dict = self.__dict__.copy()
        # remove unpickable attributes
        for attr in '_pool _rich_console _live _progress_bar'.split(' '):
            if attr in self_dict:
                del self_dict[attr]
        return self_dict
    def __setstate__(self, state):
        self.__dict__.update(state)


class Candidate:
    """Base class for search agents in all optimization methods. 
    Candidate solution for the optimization problem.
    
    Attributes
    ----------
    X : ndarray
        Design vector.
    O : ndarray
        Objectives' values.
    C : ndarray
        Constraints' values.
    f : float
        Fitness.
    
    Returns
    -------
    Candidate
        CandidateState instance.
        
    """

    def __init__(self, optimizer: Optimizer):

        if optimizer is None:
            return

        self.X = np.full(optimizer.dimensions, np.nan)
        self.O = np.full(optimizer.objectives, np.nan)
        self.C = np.full(optimizer.constraints, np.nan)
        self.f = np.nan

        # Comparison operators
        if optimizer.objectives == 1 and optimizer.constraints == 0:
            self._eq_fn = self._eq_fast
            self._lt_fn = self._lt_fast
            # self.__gt__ = self._gt_fast
        else:
            self._eq_fn = self._eq_full
            self._lt_fn = self._lt_full
            # self.__gt__ = self._gt_full

        # if optimizer.forward_unique_str:
        self.unique_str = None

    def clip(self, optimizer):
        """Method for clipping (trimming) the design vector (Candidate.X)
        values to lower and upper bounds.
        
        Returns
        -------
        None
            Nothing
            
        """

        self.X = np.clip(self.X, optimizer.lb, optimizer.ub)

    def copy(self):
        """Method for creating a copy of the CandidateState.
        
        Returns
        -------
        Candidate
            CandidateState instance
            
        """

        candidate = Candidate(None)
        candidate.X = np.copy(self.X)
        candidate.O = np.copy(self.O)
        candidate.C = np.copy(self.C)
        candidate.f = self.f

        # Comparison operators
        if self.O.size == 1 and self.C.size == 0:
            candidate._eq_fn = candidate._eq_fast
            candidate._lt_fn = candidate._lt_fast
            # self.__gt__ = self._gt_fast
        else:
            candidate._eq_fn = candidate._eq_full
            candidate._lt_fn = candidate._lt_full
            # self.__gt__ = self._gt_full

        # if optimizer.forward_unique_str:
        candidate.unique_str = self.unique_str

        return candidate

        # previous solution (much slower)
        # cP = copy.deepcopy(self)
        # return cP

    def __str__(self):
        """Method for a useful printout of Candidate properties.
        
        Returns
        -------
        printout : str
            String of the fancy table of Candidate properties.
            
        """

        title = f'Indago {type(self).__name__}'
        if type(self) != Candidate:
            title += ' (subclass of Candidate)'
        table = Table(title=title)

        table.add_column('Property', justify='left', style='magenta')
        table.add_column('Value', justify='left', style='cyan')

        for var, value in vars(self).items():
            if not var.startswith('_'):
                if isinstance(value, (int, float, str, bool)):
                    table.add_row(var, str(value))
                elif isinstance(value, (list, dict)) and len(value) > 0:
                    table.add_row(var, str(value))
                elif isinstance(value, np.ndarray) and np.size(value) > 0:
                    if isinstance(value[0], (int, float)):
                        table.add_row(var, np.array_str(value, max_line_width=np.inf))

        Console().print(table)
        return ''

    def __eq__(self, other):
        """Equality operator wrapper. 
        
        Parameters
        ----------
        other : Candidate
            The other of the two candidate solutions.
        
        Returns
        -------
        equal : bool
            ``True`` if candidate solutions are equal, ``False`` otherwise.
            
        """

        return self._eq_fn(self, other)

    @staticmethod
    def _eq_fast(a, b):
        """Private method for fast candidate solution equality check. 
        Used in single objective, unconstrained optimization.
        
        Parameters
        ----------
        a : Candidate
            The first of the two candidate solutions.
        b : Candidate
            The second of the two candidate solutions.            
        
        Returns
        -------
        equal : bool
            ``True`` if candidate solutions are equal, ``False`` otherwise.
            
        """

        return a.f == b.f

    @staticmethod
    def _eq_full(a, b):
        """Private method for full candidate solution equality check. 
        Used in multiobjective and/or constrained optimization.
        
        Parameters
        ----------
        a : Candidate
            The first of the two candidate solutions.
        b : Candidate
            The second of the two candidate solutions.            
        
        Returns
        -------
        equal : bool
            ``True`` if candidate solutions are equal, ``False`` otherwise.
            
        """

        # return np.sum(np.abs(a.X - b.X)) + np.sum(np.abs(a.O - b.O)) + np.sum(np.abs(a.C - b.C)) == 0.0
        return (a.X == b.X).all() and (a.O == b.O).all() and (a.C == b.C).all() and a.f == b.f

    def __ne__(self, other):
        """Inequality operator. 
        
        Parameters
        ----------
        other : Candidate
            The other of the two candidate solutions.
        
        Returns
        -------
        not_equal : bool
            ``True`` if candidate solutions are not equal, ``False`` otherwise.
            
        """

        return self.f != other.f

    def __lt__(self, other):
        """Less-then operator wrapper. 
        
        Parameters
        ----------
        other : Candidate
            The other of the two candidate solutions.
        
        Returns
        -------
        lower_than : bool
            ``True`` if the candidate solution is better than **other**, ``False`` otherwise.
            
        """

        return self._lt_fn(self, other)

    @staticmethod
    def _lt_fast(a, b):
        """Fast less-than operator. 
        Used in single objective, unconstrained optimization.
        
        Parameters
        ----------
        a : Candidate
            The first of the two candidate solutions.
        b : Candidate
            The second of the two candidate solutions.   
        
        Returns
        -------
        lower_than : bool
            ``True`` if **a** is better than **b**, ``False`` otherwise.
            
        """

        if np.isnan(a.f):
            return False
        if np.isnan(b.f):
            return True
        return a.f < b.f

    @staticmethod
    def _lt_full(a, b):
        """Private method for full less-than check. 
        Used in multiobjective and/or constrained optimization.
        
        Parameters
        ----------
        a : Candidate
            The first of the two candidate solutions.
        b : Candidate
            The second of the two candidate solutions.            
        
        Returns
        -------
        lower_than : bool
            ``True`` if **a** is better than **b**, ``False`` otherwise.
            
        """

        if np.isnan([*a.O, *a.C]).any():
            return False
        if np.isnan([*b.O, *b.C]).any():
            return True
        if np.sum(a.C > 0) == 0 and np.sum(b.C > 0) == 0:
            # Both are feasible
            # Better candidate is the one with smaller fitness
            return a.f < b.f

        elif np.sum(a.C > 0) == np.sum(b.C > 0):
            # Both are unfeasible and break same number of constraints
            # Better candidate is the one with smaller sum of unfeasible (positive) constraint values
            return np.sum(a.C[a.C > 0]) < np.sum(b.C[b.C > 0])

        else:
            # The number of unsatisfied constraints is not the same
            # Better candidate is the one which breaks fewer constraints
            return np.sum(a.C > 0) < np.sum(b.C > 0)

    def __gt__(self, other):
        """Greater-then operator wrapper. 
        
        Parameters
        ----------
        other : Candidate
            The other of the two candidate solutions.
        
        Returns
        -------
        greater_than : bool
            ``True`` if the candidate solution is worse than **other**, ``False`` otherwise.
            
        """

        return not (self._lt_fn(self, other) or self._eq_fn(self, other))

    # are these two necessary?
    """
    def _gt_fast(self, other):
        return self.f > other.f
    def _gt_full(self, other): 
        return not (self.__eq__(other) or self.__lt__(other))
    """

    def __le__(self, other):
        """Less-than-or-equal operator wrapper. 
        
        Parameters
        ----------
        other : Candidate
            The other of the two candidate solutions.
        
        Returns
        -------
        lower_than_or_equal : bool
            ``True`` if the candidate solution is better or equal to **other**, ``False`` otherwise.
            
        """

        return self._lt_fn(self, other) or self.__eq__(other)

    def __ge__(self, other):
        """Greater-than-or-equal operator wrapper. 
        
        Parameters
        ----------
        other : Candidate
            The other of the two candidate solutions.
        
        Returns
        -------
        greater_than_or_equal : bool
            ``True`` if the candidate solution is worse or equal to **other**, ``False`` otherwise.
            
        """

        return self.__gt__(other) or self.__eq__(other)

    def is_feasible(self):
        """
        Determines whether the design vector of a Candidate is a feasible solution. A feasible solution is
        a solution which satisfies all constraints.

        Returns
        -------
        is_feasible : bool
            ``True`` if the Candidate design vector is feasible, ``False`` otherwise.

        """

        return np.all(self.C <= 0)
